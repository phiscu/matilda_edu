{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90e24a2-d414-478d-81ad-4a69baf541d0",
   "metadata": {},
   "source": [
    "# MATILDA Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e3d49-e95d-435d-abe6-36de29ac20f6",
   "metadata": {},
   "source": [
    "After calibrating MATILDA we can now use the best parameter set to run the model with climate scenario data until 2100. In this notebook we will...\n",
    "\n",
    "- ...run MATILDA with the same parameters and settings for two emission scenarios and all models of the ensemble.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> On a single CPU one MATILDA run over 120y takes ~4s. For all ensemble members this adds up to ~4min. The <code>MatildaBulkProcessor</code> class allows you to reduce this time significantly with more CPUs so you might want to run this notebook locally. Or have a coffee. Again...</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02dcda-b1a9-4e1b-b172-f0ab8e16f415",
   "metadata": {},
   "source": [
    "## Set up the scenario runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434caf4-d9da-4ae5-903e-d7facdd9e6c6",
   "metadata": {},
   "source": [
    "As before, we start by reading our paths from the `config.ini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582b6ee5-693a-401b-ac7b-1f69b04d739a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input path: 'input/'\n",
      "Output path: 'output/'\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "# read local config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get directories from config.ini\n",
    "dir_input = config['FILE_SETTINGS']['DIR_INPUT']\n",
    "dir_output = config['FILE_SETTINGS']['DIR_OUTPUT']\n",
    "zip_output = config['CONFIG']['ZIP_OUTPUT']\n",
    "\n",
    "# set the file format for storage\n",
    "compact_files = config.getboolean('CONFIG','COMPACT_FILES')\n",
    "\n",
    "# get the number of available cores\n",
    "num_cores = int(config['CONFIG']['NUM_CORES'])\n",
    "\n",
    "print(f\"Input path: '{dir_input}'\")\n",
    "print(f\"Output path: '{dir_output}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e4aa6-a290-4801-b3ac-318285c7822e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Choose in the config between faster <code>pickle</code> files and smaller <code>parquet</code> files.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcceea2-b3a9-46d5-a403-b274269d40ed",
   "metadata": {},
   "source": [
    "Let's extend the modeling period to the full century. Therefore, we read the `settings.yaml` to a ditionary and change the respective settings. We also turn off the plotting module to reduce processing time and add the glacier profile from its `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe56f39-49b5-4c59-8a8d-662efd110483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings for MATILDA scenario runs:\n",
      "\n",
      "area_cat: 295.67484249904464\n",
      "area_glac: 31.829413146585885\n",
      "ele_cat: 3293.491688025922\n",
      "ele_dat: 3335.668840874115\n",
      "ele_glac: 4001.8798828125\n",
      "elev_rescaling: True\n",
      "freq: M\n",
      "lat: 42.18280043250193\n",
      "plot_type: all\n",
      "set_up_end: 1999-12-31\n",
      "set_up_start: 1998-01-01\n",
      "sim_end: 2100-12-31\n",
      "sim_start: 2000-01-01\n",
      "warn: False\n",
      "glacier_profile:      Elevation      Area          WE  EleZone\n",
      "0         1970  0.000000      0.0000     1900\n",
      "1         2000  0.000000      0.0000     2000\n",
      "2         2100  0.000000      0.0000     2100\n",
      "3         2200  0.000000      0.0000     2200\n",
      "4         2300  0.000000      0.0000     2300\n",
      "..         ...       ...         ...      ...\n",
      "156       4730  0.000023  20721.3700     4700\n",
      "157       4740  0.000013  14450.2180     4700\n",
      "158       4750  0.000006  10551.4730     4700\n",
      "159       4760  0.000000      0.0000     4700\n",
      "160       4780  0.000002   6084.7456     4700\n",
      "\n",
      "[161 rows x 4 columns]\n",
      "plots: False\n"
     ]
    }
   ],
   "source": [
    "from tools.helpers import read_yaml, write_yaml\n",
    "import pandas as pd\n",
    "matilda_settings = read_yaml(f\"{dir_output}/settings.yml\")\n",
    "adapted_settings = {\n",
    "    \"set_up_start\": '1998-01-01',  # Start date of the setup period\n",
    "    \"set_up_end\": '1999-12-31',  # End date of the setup period\n",
    "    \"sim_start\": '2000-01-01',  # Start date of the simulation period\n",
    "    \"sim_end\": '2100-12-31',  # End date of the simulation period\n",
    "    \"plots\": False\n",
    "}\n",
    "matilda_settings['glacier_profile'] = pd.read_csv(f\"{dir_output}/glacier_profile.csv\")\n",
    "\n",
    "matilda_settings.update(adapted_settings)\n",
    "\n",
    "print(\"Settings for MATILDA scenario runs:\\n\")\n",
    "for key in matilda_settings.keys(): print(key + ': ' + str(matilda_settings[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca1800-4f02-4a4f-9784-e050a4bb2487",
   "metadata": {},
   "source": [
    "Now, we read the calibrated parameter set from the `parameters.yml` and our forcing data from the binary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b272ac4-724c-401b-8ace-86a31b94aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters loaded.\n",
      "Forcing data loaded.\n"
     ]
    }
   ],
   "source": [
    "from tools.helpers import parquet_to_dict, pickle_to_dict\n",
    "\n",
    "param_dict = read_yaml(f\"{dir_output}/parameters.yml\")\n",
    "print(\"Parameters loaded.\")\n",
    "\n",
    "if compact_files:\n",
    "    # For size:\n",
    "    tas = parquet_to_dict(f\"{dir_output}cmip6/adjusted/tas_parquet\")\n",
    "    pr = parquet_to_dict(f\"{dir_output}cmip6/adjusted/pr_parquet\")\n",
    "else:\n",
    "    # For speed\n",
    "    tas = pickle_to_dict(f\"{dir_output}cmip6/adjusted/tas.pickle\")\n",
    "    pr = pickle_to_dict(f\"{dir_output}cmip6/adjusted/pr.pickle\")\n",
    "\n",
    "print(\"Forcing data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1297ba3-5245-4eb9-b9ca-175cf40f89fc",
   "metadata": {},
   "source": [
    "The `create_scenario_dict` function converts the individual climate projections into MATILDA input dataframes. We store the ensemble of MATILDA inputs in a nested dictionary again and save the file in a `parquet` (or `pickle`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6967a309-7dec-4b81-971c-aacb0d541e40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing MATILDA scenario input dataframes on disk...\n"
     ]
    }
   ],
   "source": [
    "from tools.helpers import dict_to_parquet, dict_to_pickle, create_scenario_dict\n",
    "\n",
    "scenarios = create_scenario_dict(tas, pr, [2, 5])\n",
    "\n",
    "print(\"Storing MATILDA scenario input dataframes on disk...\")\n",
    "\n",
    "if compact_files:\n",
    "    dict_to_parquet(scenarios, f\"{dir_output}cmip6/adjusted/matilda_scenario_input_parquet\")\n",
    "else:\n",
    "    dict_to_pickle(scenarios, f\"{dir_output}cmip6/adjusted/matilda_scenario_input.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ce469-675b-4612-8f82-bf177243c366",
   "metadata": {},
   "source": [
    "## Running MATILDA for all climate projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93b379-d00f-437f-933f-1c602bc61534",
   "metadata": {},
   "source": [
    "Now that we are set up we need to **run MATILDA for every CMIP6 model and both scenarios**. This adds up to **50-70 model runs at ~4s each** on a single core, depending on how many models remained in your ensemble. So you can either start the bulk processor and have a break or download the repo and change the config according to your available cores.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Don't be confused by the status bar. It only updates after one full scenario is processed.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f082b4d-db3d-437f-9be7-b141d13d4aba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scenarios SSP2 and SSP5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [03:37<00:00, 108.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing MATILDA scenario outputs on disk...\n",
      "Output folder can be download now (file output_download.zip)\n"
     ]
    }
   ],
   "source": [
    "from tools.helpers import MatildaBulkProcessor\n",
    "import shutil\n",
    "\n",
    "# Create an instance of the MatildaBulkProcessor class\n",
    "matilda_bulk = MatildaBulkProcessor(scenarios, matilda_settings, param_dict)\n",
    "\n",
    "# Run Matilda in a loop (takes a while - have a coffee)\n",
    "if num_cores == 1:\n",
    "    matilda_scenarios = matilda_bulk.run_single_process()\n",
    "else:\n",
    "    matilda_scenarios = matilda_bulk.run_multi_process(num_cores=num_cores)\n",
    "\n",
    "print(\"Storing MATILDA scenario outputs on disk...\")\n",
    "\n",
    "if compact_files:\n",
    "    dict_to_parquet(matilda_scenarios, f\"{dir_output}cmip6/adjusted/matilda_scenarios_parquet\")\n",
    "else:\n",
    "    dict_to_pickle(matilda_scenarios, f\"{dir_output}cmip6/adjusted/matilda_scenarios.pickle\")\n",
    "\n",
    "if zip_output:\n",
    "    # refresh `output_download.zip` with data retrieved within this notebook\n",
    "    shutil.make_archive('output_download', 'zip', 'output')\n",
    "    print('Output folder can be download now (file output_download.zip)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f932e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8537b-58ea-47b9-983a-6e0971c38c9d",
   "metadata": {},
   "source": [
    "The result is a large nested dictionary with 100-140 dataframes of MATILDA outputs. Now, it is finally time to look at the results. Explore your projections in [Notebook 6](Notebook6_Analysis.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
