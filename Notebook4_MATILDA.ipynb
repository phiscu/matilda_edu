{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0763ff34",
   "metadata": {},
   "source": [
    "# Calibrating the MATILDA framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd56f78",
   "metadata": {},
   "source": [
    "In this notebook we will\n",
    "\n",
    "1. ... set up a glacio-hydrological model with all the data we have collected,\n",
    "\n",
    "2. ... run the model for the calibration period with default parameters and check the results,\n",
    "\n",
    "3. ... use a statistical parameter optimization routine to calibrate the model,\n",
    "\n",
    "4. ... and store the calibrated parameter set for the scenario runs in the next notebook.\n",
    "\n",
    "\n",
    "The framework for Modeling water resources in glacierized catchments [MATILDA] (https://github.com/cryotools/matilda) has been developed for use in this workflow and is published as a Python package. It is based on the widely used [HBV hydrological model](https://www.cabdirect.org/cabdirect/abstract/19961904773), extended by a simple temperature-index melt model based  on the code of [Seguinot (2019)](https://zenodo.org/record/3467639). Glacier evolution over time is modeled using a modified version of the &Delta;*h* approach following [Seibert et. al. (2018)](https://doi.org/10.5194/hess-22-2211-2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420d9df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "As before we start by loading configurations such as the calibration period and some helper functions to work with  `yaml` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.helpers import update_yaml, read_yaml, write_yaml\n",
    "import configparser\n",
    "import ast\n",
    "\n",
    "# read local config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get output dir and date range from config.ini\n",
    "dir_input = config['FILE_SETTINGS']['DIR_INPUT']\n",
    "dir_output = config['FILE_SETTINGS']['DIR_OUTPUT']\n",
    "date_range = ast.literal_eval(config['CONFIG']['DATE_RANGE'])\n",
    "\n",
    "print('MATILDA will be calibrated on the period ' + date_range[0] + ' to ' + date_range[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686ab72",
   "metadata": {},
   "source": [
    "Since HBV is a so-called 'bucket' model and all the buckets are empty in the first place we need to fill them in a setup period of minimum one year. If not specified, the first two years of the `DATE_RANGE` in the `config` are used for set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "length_of_setup_period = 2\n",
    "\n",
    "sim_start = pd.to_datetime(date_range[0]) + pd.DateOffset(years = length_of_setup_period)\n",
    "set_up_end = sim_start - pd.DateOffset(days = 1)\n",
    "\n",
    "dates = {'set_up_start': date_range[0],\n",
    "         'set_up_end': str(set_up_end).split(' ')[0],        # remove hh:mm:ss\n",
    "         'sim_start': str(sim_start).split(' ')[0],          # remove hh:mm:ss\n",
    "         'sim_end': date_range[1]}\n",
    "\n",
    "for key in dates.keys(): print(key + ': ' + dates[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3810b",
   "metadata": {},
   "source": [
    "Many MATILDA parameters have been calculated in previous notebooks and stored in `settings.yaml`. We can easily add the modeling periods using a helper function. The calculated glacier profile from Notebook 1 can be imported as a `pandas DataFrame` and added to the `settings` dictionary as well.\n",
    "\n",
    "Finally, we will also add some optional settings that control the aggregation frequency of the outputs, the choice of graphical outputs, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fe70f",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "update_yaml(dir_output + 'settings.yml', dates)\n",
    "\n",
    "remaining_settings = {\"freq\": \"M\",               # aggregation frequency of model outputs (D, M, Y)\n",
    "                      \"warn\": False,             # show warnings of subpackages?\n",
    "                      \"plot_type\": \"all\",        # interactive and/or non-interactive plots ('print', 'interactive', 'all')\n",
    "                      \"elev_rescaling\": True}    # treat mean glacier elevation as constant or change with glacier evolution\n",
    "\n",
    "update_yaml(dir_output + 'settings.yml', remaining_settings)\n",
    "\n",
    "settings = read_yaml(dir_output + 'settings.yml')\n",
    "glacier_profile = pd.read_csv(dir_output + 'glacier_profile.csv')\n",
    "settings['glacier_profile'] = glacier_profile\n",
    "\n",
    "print('MATILDA settings:\\n\\n')\n",
    "for key in settings.keys(): print(key + ': ' + str(settings[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6e1aa",
   "metadata": {},
   "source": [
    "## Run MATILDA with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e908ff5",
   "metadata": {},
   "source": [
    "We will force MATILDA with the pre-processed ERA5-Land data from Notebook 2. Although MATILDA can run without calibration on observations, the results would have extreme uncertainties. Therefore, we recommend to use at least discharge observations for your selected point to evaluate the simulations against. Here, we load discharge observations for your example catchment. As you can see, we have meteorological forcing data since 1979 and discharge data since 1982. However, most of the glacier datasets used start in 2000/2001 and glaciers are a significant part of the water balance. In its current version, MATILDA is not able to extrapolate glacier cover, so we will only calibrate the model using data from 2000 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffec94",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "era5 = pd.read_csv(dir_output + 'ERA5L.csv', usecols=['dt', 'temp', 'prec'])\n",
    "era5.columns = ['TIMESTAMP', 'T2', 'RRR']\n",
    "\n",
    "# remove HH:MM:SS from 'TIMESTAMP' column\n",
    "era5['TIMESTAMP'] = pd.to_datetime(era5['TIMESTAMP'])\n",
    "era5['TIMESTAMP'] = era5['TIMESTAMP'].dt.date\n",
    "\n",
    "print('ERA5 Data:')\n",
    "display(era5)\n",
    "\n",
    "obs = pd.read_csv(dir_input + 'obs_runoff_example.csv')\n",
    "\n",
    "print('Observations:')\n",
    "display(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a953535",
   "metadata": {},
   "source": [
    "With all settings and input data in place, we can run MATILDA with **default parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff36d41",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from matilda.core import matilda_simulation\n",
    "\n",
    "output_matilda = matilda_simulation(era5, obs, **settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ed7f",
   "metadata": {},
   "source": [
    "The results are obviously far from reality and largely overestimate runoff. The **Kling-Gupta Efficiency coefficient ([KGE](https://doi.org/10.1029/2011WR010962))** rates the result as 0.36 with 1.0 being a perfect match with the observations. We can also see that the input precipitation is much higher than the total runoff. Clearly, **the model needs calibration**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bd267",
   "metadata": {},
   "source": [
    "## Calibrate MATILDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d80ae",
   "metadata": {},
   "source": [
    "In order to adjust the model parameters to the catchment characteristics, we will perform an **automated calibration**. The [MATILDA Python package](https://github.com/cryotools/matilda) contains a calibration module called *[matilda.mspot](https://github.com/cryotools/matilda/blob/master/matilda/mspot_glacier.py)* that makes extensive use of the Statistical Parameter Optimization Tool for Python ([SPOTPY](https://doi.org/10.1371/journal.pone.0145180)). As we have seen, there can be large uncertainties in the input data (especially precipitation). Simply tuning the model parameters to match the observed hydrograph may over- or underestimate other runoff contributors, such as glacier melt, to compensate for deficiencies in the precipitation data. Therefore, it is good practice to include additional data sets in a **multi-objective calibration**. In this workflow we will use:\n",
    "\n",
    "- remote sensing estimates of **glacier surface mass balance** (**SMB**) and ...\n",
    "\n",
    "- **snow water equivalent** (**SWE**) estimates from a dedicated snow reanalysis product.\n",
    "\n",
    "Unfortunately, both default datasets are **limited to High Mountain Asia (HMA)**. For study sites in other areas, please consult other sources and manually add the target values for calibration in the appropriate code cells. We are happy to include additional datasets as long as they are available online and can be integrated into the workflow.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Statistical parameter optimization (SPOT) algorithms require a high number of model runs, especially for large parameter sets. Both <i>mybinder.org</i> and <i>Google Colab</i> offer a maximum of two cores per user. One MATILDA calibration run for 20 years takes roughly 3s on one core. Therefore, large optimization runs in an online environment will be slow and may require you to leave the respective browser tab in the foreground for hours. To speed things up, you can either...</div>\n",
    "\n",
    "... run this notebook **locally on a computer with more cores** (ideally a high performance cluster) or ...\n",
    "\n",
    "... **reduce the number of calibration parameters** based the global sensitivity. We will return to this topic later in this notebook.\n",
    "\n",
    "For now, we will demonstrate how to use the SPOT features and then continue with a parameter set from a large HPC optimization run. If you need help implementing the routine on your HPC, consult the [SPOTPY documentation](https://spotpy.readthedocs.io/en/latest/Advanced_hints/#mpi-parallel-computing) and [contact us](https://github.com/phiscu/matilda_edu/issues/new) if you encounter problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f97fb",
   "metadata": {},
   "source": [
    "### Glacier surface mass balance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1dfa7",
   "metadata": {},
   "source": [
    "There are various sources of SMB records but only remote sensing estimates provide data for (almost) all glaciers in the target catchment. [Shean et. al. 2020 ](https://doi.org/10.3389/feart.2019.00363) calculated geodetic mass balances for all glaciers in High Mountain Asia from 2000 to 2018. For this example (and all other catchments in HMA), we can use their data set so derive an average annual mass balance in the calibration period.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> As all remote sensing estimates the used dataset has significant uncertainties. A comparison to other datasets and the impact on the modeling results are discussed in the associated publication. </div>\n",
    "\n",
    "We pick all individual mass balances that match the glacier IDs in our catchment and calculate the catchment-wide mean. In addition, we use the uncertainty estimate provided in the dataset to derive an uncertainty range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mass_balances = pd.read_csv(dir_input + '/hma_mb_20190215_0815_rmse.csv', usecols=['RGIId', 'mb_mwea', 'mb_mwea_sigma'])\n",
    "ids = pd.read_csv(dir_output + '/RGI/Glaciers_in_catchment.csv')\n",
    "\n",
    "merged = pd.merge(mass_balances, ids, on='RGIId')\n",
    "mean_mb = round(merged['mb_mwea'].mean() * 1000, 3)   # Mean catchment MB in mm w.e.\n",
    "mean_sigma = round(merged['mb_mwea_sigma'].mean() * abs(mean_mb), 3)  # Mean uncertainty of catchment MB in mm w.e.\n",
    "\n",
    "target_mb = [mean_mb - mean_sigma, mean_mb + mean_sigma]\n",
    "\n",
    "print('Target glacier mass balance for calibration: ' + str(mean_mb) + ' +-' + str(mean_sigma) + 'mm w.e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011562798313276",
   "metadata": {},
   "source": [
    "### Snow water equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f23fd0e3cd0088",
   "metadata": {},
   "source": [
    "For snow cover estimates, we will use a specialized **snow reanalysis** dataset from [Liu et. al. 2021](https://doi.org/10.5067/HNAUGJQXSCVU). Details on the data can be found in the dataset documentation and the associated [publication](https://doi.org/10.1029/2022GL100082).\n",
    "Unfortunately, access to the dataset requires a (free) registration at **NASA's EarthData** portal, which prevents a seamless integration. Also, the dataset consists of large files and requires some pre-processing that **could not be done in a Jupyter Notebook**. However, we provide you with the `SWEETR` tool, a **fully automated workflow** that you can run on your local computer to download, process, and aggregate the data. Please refer to the dedicated [Github repository](https://github.com/phiscu/hma_snow) for further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80bc2f3e2383de",
   "metadata": {},
   "source": [
    "<img src=\"images/mean_swe_example.png\" alt=\"Mean_SWE\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c602b60f8cf2d1",
   "metadata": {},
   "source": [
    "When you run the `SWEETR` tool with your catchment outline, it returns cropped raster files as the one shown above and, more importantly, a **timeseries of catchment-wide daily mean SWE** from 1999 to 2016. Just replace the `input/swe.csv` file with your result. Now we can load the timeseries as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5944350ddce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = pd.read_csv(f'{dir_input}/swe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ba3a61288e54f",
   "metadata": {},
   "source": [
    "Along with the cropped SWE rasters the `SWEETR` tool creates binary masks for seasonal- and non-seasonal snow. Due to its strong signal in remote sensing data, seasonal snow can be better detected leading to more robust SWE estimates. However, the non-seasonal snow largely agrees with the glacierized area. Therefore, we will calibrate the snow routine by comparing the SWE of the ice-free sub-catchment with the seasonal snow of the reanalysis. Since the latter has a coarse resolution of 500 m, the excluded catchment area is a bit larger than the RGI glacier outlines (17.2% non-seasonal snow vs. 10.8% glacierized sub-catchment). Therefore, we use a scaling factor to account for this mismatch in the reference area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b43f5b0d8d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glac_ratio = settings['area_glac'] / settings['area_cat']       # read glacieriezed and total area from the settings\n",
    "swe_area_sim = 1-glac_ratio\n",
    "swe_area_obs = 0.828            # 1 - non-seasonal snow / seasonal snow\n",
    "sf = swe_area_obs / swe_area_sim\n",
    "print('SWE scaling factor: ' + str(round(sf, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09600c01",
   "metadata": {},
   "source": [
    "The MATILDA framework provides an interface for [SPOTPY](https://github.com/thouska/spotpy/). Here we will use the `psample()` function to run MATILDA with the same settings as before but varying parameters. To do this, we will remove redundant `settings` and add some new ones specific to the function. Be sure to choose the number of repetitions carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0c953",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from tools.helpers import drop_keys\n",
    "\n",
    "psample_settings = drop_keys(settings, ['warn', 'plots', 'plot_type'])\n",
    "\n",
    "additional_settings = {'rep': 10,                             # Number of model runs. For advice, check the documentation of the algorithms.\n",
    "                       'glacier_only': False,                 # True when calibrating an entirely glacierized catchment\n",
    "                       'obj_dir': 'maximize',                 # should your objective function be maximized (e.g. NSE) or minimized (e.g. RMSE)\n",
    "                       'target_mb': mean_mb,                  # Average annual glacier mass balance to target at\n",
    "                       'target_swe': swe,                     # Catchment-wide mean SWE timeseries of seasonal snow to calibrate the snow routine\n",
    "                       'swe_scaling': 0.928,                  # scaling factor for simulated SWE to account for reference area mismatch\n",
    "                       'dbformat': None,                      # Write the results to a file ('csv', 'hdf5', 'ram', 'sql')\n",
    "                       'output': None,                        # Choose where to store the files\n",
    "                       'algorithm': 'lhs',                    # Choose algorithm (for parallelization: mc, lhs, fast, rope, sceua or demcz)\n",
    "                       'dbname': 'era5_matilda_example',      # Choose name\n",
    "                       \n",
    "                       'parallel': False,                     # Distribute the calculation on multiple cores or not\n",
    "                      # 'cores': 20                           # Set number of cores when running in parallel\n",
    "                      }\n",
    "psample_settings.update(additional_settings)\n",
    "\n",
    "print('Settings for calibration runs:\\n')\n",
    "for key in psample_settings.keys(): print(key + ': ' + str(psample_settings[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979aee1d",
   "metadata": {},
   "source": [
    "With these settings we can start the `psample()` to run our model with various parameter combinations. The default parameter boundaries can be found in the MATILDA [parameter documentation](https://github.com/cryotools/matilda/tree/master?tab=readme-ov-file#parameter-list). If you want to narrow down the parameter space, you can do that using the following syntax. Here, we define custom ranges for the temperature lapse rate and the precipitation correction factor and run a short Latin Hypercube Sampling (LHS) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7898ddc",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from matilda.mspot_glacier import psample\n",
    "\n",
    "lim_dict = {'lr_temp_lo': -0.007, 'lr_temp_up': -0.005, 'PCORR_lo': 0.5, 'PCORR_up': 1.5}\n",
    "\n",
    "best_summary = psample(df=era5, obs=obs, **psample_settings, **lim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1dd9cc8c0feec8",
   "metadata": {},
   "source": [
    "## Process-based calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325ce943566d93b",
   "metadata": {},
   "source": [
    "Every parameter governs a different aspect of the water balance, and not all parameters affect every calibration variable. Therefore, we propose an **iterative process-based calibration** approach where we calibrate parameters in order of their importance using different algorithms, objective functions, and calibration variables. Details on the calibration strategy can be found in the model publication which is currently under review. The individual steps will be implemented in the notebook soon.\n",
    "\n",
    "<img src=\"images/calibration_strategy.png\" alt=\"calibration\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1abdbb",
   "metadata": {},
   "source": [
    "## Run MATILDA with calibrated parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6221a8",
   "metadata": {},
   "source": [
    "The following parameter set was computed applying the mentioned calibration strategy on an HPC cluster.\n",
    "<a id=\"param\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'RFS': 0.15,\n",
    "    'SFCF': 1,\n",
    "    'CET': 0,\n",
    "    'PCORR': 0.58,\n",
    "    'lr_temp': -0.006,\n",
    "    'lr_prec': 0.0015,\n",
    "    'TT_diff': 0.76198,\n",
    "    'TT_snow': -1.44646,\n",
    "    'CFMAX_snow': 3.3677,\n",
    "    'BETA': 1.0,\n",
    "    'FC': 99.15976,\n",
    "    'K0': 0.01,\n",
    "    'K1': 0.01,\n",
    "    'K2': 0.15,\n",
    "    'LP': 0.998,\n",
    "    'MAXBAS': 2.0,\n",
    "    'PERC': 0.09232826,\n",
    "    'UZL': 126.411575,\n",
    "    'CFMAX_rel': 1.2556936,\n",
    "    'CWH': 0.000117,\n",
    "    'AG': 0.54930484\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print('Calibrated parameter set:\\n\\n')\n",
    "for key in param.keys(): print(key + ': ' + str(param[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a02b393237f6e",
   "metadata": {},
   "source": [
    "Properly calibrated, the model shows a much better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f194c5",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "output_matilda = matilda_simulation(era5, obs, **settings, parameter_set=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780016b",
   "metadata": {},
   "source": [
    "In addition to the standard plots we can explore the results interactive `ploty` plots. Go ahead and zoom as you like or select/deselect individual curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_matilda[9].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087168b1",
   "metadata": {},
   "source": [
    "The same works for the long-term seasonal cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_matilda[10].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a094db7",
   "metadata": {},
   "source": [
    "## Reducing the parameter space - Sensitivity analysis with FAST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1ddb1",
   "metadata": {},
   "source": [
    "To reduce the computation time of the calibration procedure, we need to reduce the number of parameters to be optimized. Therefore, we will perform a global sensitivity analysis to identify the most important parameters and set the others to default values. The algorithm of choice will be the [Fourier Amplitude Sensitivity Test (FAST)](https://www.tandfonline.com/doi/abs/10.1080/00401706.1999.10485594) available through the [SPOTPY](https://github.com/thouska/spotpy/blob/master/src/spotpy/algorithms/fast.py) library. As before, we will show the general procedure with a few iterations, but display results from extensive runs on a HPC. You can use the results as a guide for your parameter choices, but keep in mind that they are highly correlated with your catchment properties, such as elevation range and glacier coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d968996",
   "metadata": {},
   "source": [
    "First, we calculate the required number of iterations using a formula from [Henkel et. al. 2012](https://www.informs-sim.org/wsc12papers/includes/files/con308.pdf). We choose the SPOTPY default frequency step of 2 and set the interference factor to a maximum of 4, since we assume a high intercorrelation of the model parameters. The total number of parameters in MATILDA is 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfca7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_iter(param, interf=4, freqst=2):\n",
    "    \"\"\"\n",
    "    Calculates the number of parameter iterations needed for parameterization and sensitivity analysis using FAST.\n",
    "    Parameters\n",
    "    ----------\n",
    "    param : int\n",
    "        The number of input parameters being analyzed.\n",
    "    interf : int\n",
    "        The interference factor, which determines the degree of correlation between the input parameters.\n",
    "    freqst : int\n",
    "        The frequency step, which specifies the size of the intervals between each frequency at which the Fourier transform is calculated.\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The total number of parameter iterations needed for FAST.\n",
    "    \"\"\"\n",
    "    return (1 + 4 * interf ** 2 * (1 + (param - 2) * freqst)) * param\n",
    "\n",
    "print('Needed number of iterations for FAST: ' + str(fast_iter(21)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf29bd",
   "metadata": {},
   "source": [
    "That is a lot of iterations! Running this routine on a single core would take about two days, but can be sped up significantly with each additional core. The setup would look exactly like the parameter optimization before.\n",
    "\n",
    "**Note:** No matter what number of iterations you define, SPOTPY will run $N*k$ times, where $k$ is the number of model parameters. So even if we set `rep=10`, the algorithm will run at least 21 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c6547",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from matilda.mspot_glacier import psample\n",
    "\n",
    "fast_settings = {'rep': 52437,                              # Choose wisely before running\n",
    "                 'target_mb': None,\n",
    "                 'algorithm': 'fast',\n",
    "                 'dbname': 'fast_matilda_example',\n",
    "                 'dbname': dir_output + 'fast_example',\n",
    "                 'dbformat': 'csv'\n",
    "                      }\n",
    "psample_settings.update(fast_settings)\n",
    "\n",
    "print('Settings for FAST:\\n\\n')\n",
    "for key in psample_settings.keys(): print(key + ': ' + str(psample_settings[key]))\n",
    "\n",
    "# fast_results = psample(df=era5, obs=obs, **psample_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825cba2f",
   "metadata": {},
   "source": [
    "We ran *FAST*s for the example catchment with the full number of iterations required. The results are saved in *CSV* files. We can use the `spotpy.analyser` library to create easy-to-read data frames from the databases. The summary shows the first (`S1`) and the total order sensitivity index (`ST`) for each parameter. `S1` refers to the variance of the model output explained by the parameter, holding all other parameters constant. The `ST` takes into account the interaction of the parameters and is therefore a good measure of the impact of individual parameters on the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotpy\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "def get_si(fast_results: str, to_csv: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes the sensitivity indices of a given FAST simulation results file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    fast_results : str\n",
    "        The path of the FAST simulation results file.\n",
    "    to_csv : bool, optional\n",
    "        If True, the sensitivity indices are saved to a CSV file with the same\n",
    "        name as fast_results, but with '_sensitivity_indices.csv' appended to\n",
    "        the end (default is False).\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A pandas DataFrame containing the sensitivity indices and parameter\n",
    "        names.\n",
    "    \"\"\"\n",
    "    if fast_results.endswith(\".csv\"):\n",
    "        fast_results = fast_results[:-4]  # strip .csv\n",
    "    results = spotpy.analyser.load_csv_results(fast_results)\n",
    "    # Suppress prints\n",
    "    with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "        SI = spotpy.analyser.get_sensitivity_of_fast(results, print_to_console=False)\n",
    "    parnames = spotpy.analyser.get_parameternames(results)\n",
    "    sens = pd.DataFrame(SI)\n",
    "    sens['param'] = parnames\n",
    "    sens.set_index('param', inplace=True)\n",
    "    if to_csv:\n",
    "        sens.to_csv(os.path.basename(fast_results) + '_sensitivity_indices.csv', index=False)\n",
    "    return sens\n",
    "\n",
    "display(get_si(dir_input + 'FAST/' + 'example_fast_nolim.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b7df1",
   "metadata": {},
   "source": [
    "If you have additional information on certain parameters, limiting their bounds can have a large impact on sensitivity. For our example catchment, field observations showed that the temperature lapse rate and precipitation correction were unlikely to exceed a certain range, so we limited the parameter space for both and ran a *FAST* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279525b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_dict = {'lr_temp_lo': -0.007, 'lr_temp_up': -0.005, 'PCORR_lo': 0.5, 'PCORR_up': 1.5}\n",
    "# fast_results = psample(df=era5, obs=obs, **psample_settings, **lim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972f31d",
   "metadata": {},
   "source": [
    "To see the effect of parameter restrictions on sensitivity, we can plot the indices of both runs. Feel free to explore further by adding more *FAST* outputs to the plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_sensitivity_bars(*dfs, labels=None, show=False, bar_width=0.3, bar_gap=0.6):\n",
    "    \"\"\"\n",
    "    Plots a horizontal bar chart showing the total sensitivity index for each parameter in a MATILDA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *dfs : pandas.DataFrame\n",
    "        Multiple dataframes containing the sensitivity indices for each parameter.\n",
    "    labels : list of str, optional\n",
    "        Labels to use for the different steps in the sensitivity analysis. If not provided, the default\n",
    "        labels will be 'No Limits', 'Step 1', 'Step 2', etc.\n",
    "    bar_width : float, optional\n",
    "        Width of each bar in the chart.\n",
    "    bar_gap : float, optional\n",
    "        Space between bars.\n",
    "    \"\"\"\n",
    "    traces = []\n",
    "    colors = ['darkblue', 'orange', 'purple', 'cyan']   # add more colors if needed\n",
    "    for i, df in enumerate(dfs):\n",
    "        df = get_si(df)\n",
    "        if i > 0:\n",
    "            if labels is None:\n",
    "                label = 'Step ' + str(i)\n",
    "            else:\n",
    "                label = labels[i]\n",
    "        else:\n",
    "            label = 'No Limits'\n",
    "        trace = go.Bar(y=df.index,\n",
    "                       x=df['ST'],\n",
    "                       name=label,\n",
    "                       orientation='h',\n",
    "                       marker=dict(color=colors[i]),\n",
    "                       width=bar_width)\n",
    "        traces.append(trace)\n",
    "    layout = go.Layout(title=dict(text='<b>' +'Total Sensitivity Index for MATILDA Parameters' + '<b>', font=dict(size=24)),\n",
    "                   xaxis_title='Total Sensitivity Index',\n",
    "                   yaxis_title='Parameter',\n",
    "                   yaxis=dict(automargin=True),\n",
    "                   bargap=bar_gap,\n",
    "                   height=700)\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    if show:\n",
    "        fig.show()\n",
    "        \n",
    "step1 = dir_input + 'FAST/' + 'example_fast_nolim.csv'\n",
    "step2 = dir_input + 'FAST/' + 'era5_ipynb_fast_step1.csv'\n",
    "\n",
    "plot_sensitivity_bars(step1, step2, labels=['No Limits', 'lr_temp and PCORR limited'], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae14b0",
   "metadata": {},
   "source": [
    "From this figure, we can easily identify the most important parameters. Depending on your desired accuracy and computational resources, you can choose a sensitivity threshold. Let's say we want to fix all parameters with a sensitivity index below 0.1. For our example, this will reduce the calibration parameters to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed28f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_df = get_si(step2)\n",
    "sensitive = si_df.index[si_df['ST'] > 0.10].values\n",
    "\n",
    "print('Parameters with a sensitivity index > 0.1:')\n",
    "for i in sensitive: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d8ff3",
   "metadata": {},
   "source": [
    "To give you an idea how much this procedure can reduce calibration time, we run the `fast_iter()` function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Needed number of iterations for FAST with all parameters: ' + str(fast_iter(21)))\n",
    "print('Needed number of iterations for FAST with the 7 most sensitive parameters: ' + str(fast_iter(7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5c14b",
   "metadata": {},
   "source": [
    "For a single core this would roughly translate into a reduction from 44h to 4h."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0728af",
   "metadata": {},
   "source": [
    "To run the calibration with the reduced parameter space, we simply need to define values for the fixed parameters and use a helper function to translate them into boundary arguments. Here we simply use the values from our last *SPOTPY* run (`param`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matilda.mspot_glacier import dict2bounds\n",
    "\n",
    "fixed_param = {key: val for key, val in param.items() if key not in sensitive}\n",
    "fixed_param_bounds = dict2bounds(fixed_param)\n",
    "       \n",
    "print('Fixed bounds:\\n')\n",
    "for key in fixed_param_bounds.keys(): print(key + ': ' + str(fixed_param_bounds[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7348e",
   "metadata": {},
   "source": [
    "The `psample()` setup is then as simple as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86a06d",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "new_settings = {'rep': 10,                             # Number of model runs. For advice check the documentation of the algorithms.\n",
    "                'glacier_only': False,                 # True when calibrating a entirely glacierized catchment\n",
    "                'obj_dir': 'maximize',                 # should your objective funtion be maximized (e.g. NSE) or minimized (e.g. RMSE)\n",
    "                'target_mb': -156,                     # Average annual glacier mass balance to target at\n",
    "                'dbformat': None,                      # Write the results to a file ('csv', 'hdf5', 'ram', 'sql')\n",
    "                'output': None,                        # Choose where to store the files\n",
    "                'algorithm': 'lhs',                    # Choose algorithm (for parallelization: mc, lhs, fast, rope, sceua or demcz)\n",
    "                'dbname': 'era5_matilda_example',      # Choose name\n",
    "                       \n",
    "                'parallel': False,                     # Distribute the calculation on multiple cores or not\n",
    "                # 'cores': 20                           # Set number of cores when running parallel\n",
    "                      }\n",
    "psample_settings.update(new_settings)\n",
    "\n",
    "best_summary = psample(df=era5, obs=obs, **psample_settings, **fixed_param_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea5806",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The number of iterations required depends on the selected algorithm and the number of free parameters. To choose the correct setting, consult the <a href='https://spotpy.readthedocs.io/en/latest/Algorithm_guide/'>SPOTPY documentation</a> and the original literature for each algorithm.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938d472",
   "metadata": {},
   "source": [
    "Now, we save the best parameter set for use in the next notebook. The set is stored in the `best_summary` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3f10b",
   "metadata": {},
   "source": [
    "**Note:** In our example we will skip this step and use the [result from the calibration on an HPC cluster](#param)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = best_summary['best_param']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68a349",
   "metadata": {},
   "source": [
    "Finally, we store the parameter set in a `.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_yaml(param, dir_output + 'parameters.yml')\n",
    "print(f\"Parameter set stored in '{dir_output}parameters.yml'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# refresh `output_download.zip` with data retrieved within this notebook\n",
    "shutil.make_archive('output_download', 'zip', 'output')\n",
    "print('Output folder can be download now (file output_download.zip)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
