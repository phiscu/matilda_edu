{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866e2c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Google Earth Engine packages\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a38c0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize GEE at the beginning of session\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()         # authenticate when using GEE for the first time\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f7b68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import ast\n",
    "\n",
    "# read local config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get file config from config.ini\n",
    "dir_output = config['FILE_SETTINGS']['DIR_OUTPUT']\n",
    "output_gpkg = dir_output + config['FILE_SETTINGS']['GPKG_NAME']\n",
    "\n",
    "# get date range for forcing data\n",
    "#date_range = ast.literal_eval(config['CONFIG']['DATE_RANGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c172b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "catchment_new = gpd.read_file(output_gpkg, layer='catchment_new')\n",
    "catchment = geemap.geopandas_to_ee(catchment_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ae718",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3971671",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def renameBandName(b):\n",
    "    split = ee.String(b).split('_')   \n",
    "    return ee.String(split.splice(split.length().subtract(2),1).join(\"_\"))\n",
    "\n",
    "\n",
    "def buildFeature(i):\n",
    "    t1 = startDate.advance(i,'day')\n",
    "    t2 = t1.advance(1,'day')\n",
    "    #feature = ee.Feature(point)\n",
    "    dailyColl = collection.filterDate(t1, t2)\n",
    "    dailyImg = dailyColl.toBands()\n",
    "    # renaming and handling names\n",
    "    bands = dailyImg.bandNames()\n",
    "    renamed = bands.map(renameBandName)\n",
    "    # Daily extraction and adding time information\n",
    "    dict = dailyImg.rename(renamed).reduceRegion(\n",
    "      reducer=ee.Reducer.mean(),\n",
    "      geometry=catchment,\n",
    "    ).combine(\n",
    "      ee.Dictionary({'system:time_start':t1.millis(),'isodate':t1.format('YYYY-MM-dd')})\n",
    "    )\n",
    "    return ee.Feature(None,dict)\n",
    "\n",
    "\n",
    "def getImageCollection(var):\n",
    "    collection = ee.ImageCollection('NASA/GDDP-CMIP6')\\\n",
    "        .select(var)\\\n",
    "        .filterDate(startDate, endDate)\\\n",
    "        .filterBounds(catchment)\n",
    "    return collection\n",
    "\n",
    "\n",
    "def getTask(fileName):\n",
    "    task = ee.batch.Export.table.toDrive(**{\n",
    "      'collection': ee.FeatureCollection(ee.List.sequence(0,n).map(buildFeature)),\n",
    "      'description':fileName,\n",
    "      'fileFormat': 'CSV'\n",
    "    })\n",
    "    return task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb96f98",
   "metadata": {},
   "source": [
    "# Set periods for climate scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48377c",
   "metadata": {},
   "source": [
    "To provide the best basis for bias adjustment a large overlap of reanalysis and scenario data is recommended. Per default the routine downloads scenario data starting with the earliest date available from ERA5-Land in 1979 and until 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa8e2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start = '1979-01-01'\n",
    "end = '2100-12-31'                      # exclusive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01b004",
   "metadata": {},
   "source": [
    "# Launch download tasks for scenario data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889e26c",
   "metadata": {},
   "source": [
    "CMIP6 scenario runs start in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ddf73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "startDate = ee.Date('2015-01-01')\n",
    "endDate = ee.Date(end)\n",
    "n = endDate.difference(startDate,'day').subtract(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d02af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "collection = getImageCollection('tas')\n",
    "task_tas_ssp = getTask('CMIP6_tas_ssp')\n",
    "task_tas_ssp.start()\n",
    "\n",
    "collection = getImageCollection('pr')\n",
    "task_pr_ssp = getTask('CMIP6_pr_ssp')\n",
    "task_pr_ssp.start()\n",
    "\n",
    "print('Tasks for scenarios started...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1f66b",
   "metadata": {},
   "source": [
    "# Launch download tasks for historical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005b600",
   "metadata": {},
   "source": [
    "**Caution** - Depending on the selected period, the number of models, Googles server utilization, and other mysterious factors this might take some time. The downloaded files will not exceed 100MB so the bandwidth should not be the problem. As a rough estimate you can plan with 45min for all 122 years and 34 models. In the meantime you can continue in the [next notebook](http://localhost:8888/lab/tree/Seafile/EBA-CA/Repositories/matilda_edu/MATILDA.ipynb) with calibrating MATILDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d7f73",
   "metadata": {},
   "source": [
    "The CMIP6 historical runs are available for the period of 1950 through 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22264b41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "startDate = ee.Date(start)\n",
    "endDate = ee.Date('2014-12-31')\n",
    "n = endDate.difference(startDate,'day').subtract(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba4a33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "collection = getImageCollection('tas')\n",
    "task_tas_hist = getTask('CMIP6_tas_hist')\n",
    "task_tas_hist.start()\n",
    "\n",
    "collection = getImageCollection('pr')\n",
    "task_pr_hist = getTask('CMIP6_pr_hist')\n",
    "task_pr_hist.start()\n",
    "\n",
    "print('Tasks for historical data started...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a75f3",
   "metadata": {},
   "source": [
    "Start status animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d1a0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while task_tas_ssp.active() or task_pr_ssp.active() or task_tas_hist.active() or task_pr_hist.active():\n",
    "    print(\".\", end = '')\n",
    "    time.sleep(2)\n",
    "    \n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38781b",
   "metadata": {},
   "source": [
    "# Wie bekommen wir die Daten vom Drive in den Binder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924f3f0",
   "metadata": {},
   "source": [
    "- create ID\n",
    "- direct download to new folder with ID name\n",
    "- donload the whole folder using the gdown package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7d186",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#import gdown\n",
    "\n",
    "#url = 'https://drive.google.com/drive/folders/1PHEZMh-hJrOS305qHYBCICWap91ITBIE?usp=share_link'\n",
    "#gdown.download_folder(url, quiet=True, use_cookies=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210f27d-52c3-48a4-8ce7-a2c5c23d27a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test der neuen Download-Routine mit parallelen Download-Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41cf5a4-bc87-4fd7-a7a0-92dae7645d8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import geopandas as gpd\n",
    "import concurrent.futures\n",
    "import os\n",
    "import requests\n",
    "from retry import retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CMIPDownloader:\n",
    "    \"\"\"Class to download spatially averaged CMIP6 data for a given period, variable, and spatial subset.\"\"\"\n",
    "\n",
    "    def __init__(self, var, starty, endy, shape, processes=10, dir='./'):\n",
    "        self.var = var\n",
    "        self.starty = starty\n",
    "        self.endy = endy\n",
    "        self.shape = shape\n",
    "        self.processes = processes\n",
    "        self.directory = dir\n",
    "\n",
    "        # create the download directory if it doesn't exist\n",
    "        if not os.path.exists(self.directory):\n",
    "            os.makedirs(self.directory)\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Runs a subset routine for CMIP6 data on GEE servers to create ee.FeatureCollections for all years in\n",
    "        the requested period. Downloads individual years in parallel processes to increase the download time.\"\"\"\n",
    "\n",
    "        def getRequests(starty, endy):\n",
    "            \"\"\"Generates a list of years to be downloaded. [Client side]\"\"\"\n",
    "\n",
    "            return [i for i in range(starty, endy+1)]\n",
    "\n",
    "        @retry(tries=10, delay=1, backoff=2)\n",
    "        def getResult(index, year):\n",
    "            \"\"\"Handle the HTTP requests to download one year of CMIP6 data. [Server side]\"\"\"\n",
    "\n",
    "            start = str(year) + '-01-01'\n",
    "            end = str(year + 1) + '-01-01'\n",
    "            startDate = ee.Date(start)\n",
    "            endDate = ee.Date(end)\n",
    "            n = endDate.difference(startDate, 'day').subtract(1)\n",
    "\n",
    "            def getImageCollection(var):\n",
    "                \"\"\"Create and image collection of CMIP6 data for the requested variable, period, and region.\n",
    "                [Server side]\"\"\"\n",
    "\n",
    "                collection = ee.ImageCollection('NASA/GDDP-CMIP6') \\\n",
    "                    .select(var) \\\n",
    "                    .filterDate(startDate, endDate) \\\n",
    "                    .filterBounds(self.shape)\n",
    "                return collection\n",
    "\n",
    "            def renameBandName(b):\n",
    "                \"\"\"Edit variable names for better readability. [Server side]\"\"\"\n",
    "\n",
    "                split = ee.String(b).split('_')\n",
    "                return ee.String(split.splice(split.length().subtract(2), 1).join(\"_\"))\n",
    "\n",
    "            def buildFeature(i):\n",
    "                \"\"\"Create an area weighted average of the defined region for every day in the given year.\n",
    "                [Server side]\"\"\"\n",
    "\n",
    "                t1 = startDate.advance(i, 'day')\n",
    "                t2 = t1.advance(1, 'day')\n",
    "                # feature = ee.Feature(point)\n",
    "                dailyColl = collection.filterDate(t1, t2)\n",
    "                dailyImg = dailyColl.toBands()\n",
    "                # renaming and handling names\n",
    "                bands = dailyImg.bandNames()\n",
    "                renamed = bands.map(renameBandName)\n",
    "                # Daily extraction and adding time information\n",
    "                dict = dailyImg.rename(renamed).reduceRegion(\n",
    "                    reducer=ee.Reducer.mean(),\n",
    "                    geometry=self.shape,\n",
    "                ).combine(\n",
    "                    ee.Dictionary({'system:time_start': t1.millis(), 'isodate': t1.format('YYYY-MM-dd')})\n",
    "                )\n",
    "                return ee.Feature(None, dict)\n",
    "\n",
    "            # Create features for all days in the respective year. [Server side]\n",
    "            collection = getImageCollection(self.var)\n",
    "            year_feature = ee.FeatureCollection(ee.List.sequence(0, n).map(buildFeature))\n",
    "\n",
    "            # Create a download URL for a CSV containing the feature collection. [Server side]\n",
    "            url = year_feature.getDownloadURL()\n",
    "\n",
    "            # Handle downloading the actual csv for one year. [Client side]\n",
    "            r = requests.get(url, stream=True)\n",
    "            if r.status_code != 200:\n",
    "                r.raise_for_status()\n",
    "            filename = os.path.join(self.directory, 'cmip6_' + self.var + '_' + str(year) + '.csv')\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(r.text)\n",
    "\n",
    "            return index\n",
    "\n",
    "        # Create a list of years to be downloaded. [Client side]\n",
    "        items = getRequests(self.starty, self.endy)\n",
    "\n",
    "        # Launch download requests in parallel processes and display a status bar. [Client side]\n",
    "        with tqdm(total=len(items), desc=\"Downloading CMIP6 data for variable '\" + self.var + \"'\") as pbar:\n",
    "            results = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.processes) as executor:\n",
    "                for i, year in enumerate(items):\n",
    "                    results.append(executor.submit(getResult, i, year))\n",
    "                for future in concurrent.futures.as_completed(results):\n",
    "                    index = future.result()\n",
    "                    pbar.update(1)\n",
    "\n",
    "        print(\"All downloads complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f173066-112a-4ce2-bba6-f614d923abb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cmip_dir = dir_output + 'cmip6/'\n",
    "downloader_t = CMIPDownloader('tas', 1979, 2100, catchment, processes=25, dir=cmip_dir)\n",
    "downloader_t.download()\n",
    "downloader_p = CMIPDownloader('pr', 1979, 2100, catchment, processes=25, dir=cmip_dir)\n",
    "downloader_p.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b1f8f-8525-460e-95a7-dbf3d90e609d",
   "metadata": {},
   "source": [
    "# Process the downloaded CSV files --> only works for full period (1979-2100) so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5221d-a675-427f-a3a9-890a62e390e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CMIPProcessor:\n",
    "    \"\"\"Class to read and pre-process CSV files downloaded by the CMIPDownloader class.\"\"\"\n",
    "    def __init__(self, var, dir='.'):\n",
    "        self.dir = dir\n",
    "        self.var = var\n",
    "        self.df_hist = self.append_df(self.var, self.dir, hist=True)\n",
    "        self.df_ssp = self.append_df(self.var, self.dir, hist=False)\n",
    "        self.ssp2_common, self.ssp5_common, self.hist_common,\\\n",
    "            self.common_models, self.dropped_models = self.process_dataframes()\n",
    "        self.ssp2, self.ssp5 = self.get_results()\n",
    "\n",
    "    def read_cmip(self, filename):\n",
    "        \"\"\"Reads CMIP6 CSV files and drops redundant columns.\"\"\"\n",
    "\n",
    "        df = pd.read_csv(filename, index_col='isodate', parse_dates=['isodate'])\n",
    "        df = df.drop(['system:index', '.geo', 'system:time_start'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def append_df(self, var, dir='.', hist=True):\n",
    "        \"\"\"Reads CMIP6 CSV files of individual years and concatenates them into dataframes for the full downloaded\n",
    "        period. Historical and scenario datasets are treated separately. Drops a model with data gaps.\n",
    "        Converts precipitation unit to mm.\"\"\"\n",
    "\n",
    "        df_list = []\n",
    "        if hist:\n",
    "            starty = 1979\n",
    "            endy = 2014\n",
    "        else:\n",
    "            starty = 2015\n",
    "            endy = 2100\n",
    "        for i in range(starty, endy + 1):\n",
    "            filename = dir + 'cmip6_' + var + '_' + str(i) + '.csv'\n",
    "            df_list.append(self.read_cmip(filename))\n",
    "        if hist:\n",
    "            hist_df = pd.concat(df_list).drop('historical_GFDL-CM4_' + var, axis=1)\n",
    "            if var == 'pr':\n",
    "                hist_df = hist_df * 86400       # from kg/(m^2*s) to mm/day\n",
    "            return hist_df\n",
    "        else:\n",
    "            ssp_df = pd.concat(df_list).drop(['ssp585_GFDL-CM4_' + var, 'ssp245_GFDL-CM4_' + var], axis=1)\n",
    "            if var == 'pr':\n",
    "                ssp_df = ssp_df * 86400       # from kg/(m^2*s) to mm/day\n",
    "            return ssp_df\n",
    "\n",
    "    def process_dataframes(self):\n",
    "        \"\"\"Separates the two scenarios and drops models not available for both scenarios and the historical period.\"\"\"\n",
    "\n",
    "        ssp2 = self.df_ssp.loc[:, self.df_ssp.columns.str.startswith('ssp245')]\n",
    "        ssp5 = self.df_ssp.loc[:, self.df_ssp.columns.str.startswith('ssp585')]\n",
    "        hist = self.df_hist.loc[:, self.df_hist.columns.str.startswith('historical')]\n",
    "\n",
    "        ssp2.columns = ssp2.columns.str.lstrip('ssp245_').str.rstrip('_' + self.var)\n",
    "        ssp5.columns = ssp5.columns.str.lstrip('ssp585_').str.rstrip('_' + self.var)\n",
    "        hist.columns = hist.columns.str.lstrip('historical_').str.rstrip('_' + self.var)\n",
    "\n",
    "        # Get all the models the three datasets have in common\n",
    "        common_models = set(ssp2.columns).intersection(ssp5.columns).intersection(hist.columns)\n",
    "\n",
    "        # Get the model names that contain NaN values\n",
    "        nan_models_list = [df.columns[df.isna().any()].tolist() for df in [ssp2, ssp5, hist]]\n",
    "        # flatten the list\n",
    "        nan_models = [col for sublist in nan_models_list for col in sublist]\n",
    "        # remove duplicates\n",
    "        nan_models = list(set(nan_models))\n",
    "\n",
    "        # Remove models with NaN values from the list of common models\n",
    "        common_models = [x for x in common_models if x not in nan_models]\n",
    "\n",
    "        ssp2_common = ssp2.loc[:, common_models]\n",
    "        ssp5_common = ssp5.loc[:, common_models]\n",
    "        hist_common = hist.loc[:, common_models]\n",
    "\n",
    "        dropped_models = list(set([mod for mod in ssp2.columns if mod not in common_models] +\n",
    "                                  [mod for mod in ssp5.columns if mod not in common_models] +\n",
    "                                  [mod for mod in hist.columns if mod not in common_models]))\n",
    "\n",
    "        return ssp2_common, ssp5_common, hist_common, common_models, dropped_models\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Concatenates historical and scenario data to combined dataframes of the full downloaded period.\"\"\"\n",
    "\n",
    "        ssp2_full = pd.concat([self.hist_common, self.ssp2_common])\n",
    "        ssp2_full.index.names = ['TIMESTAMP']\n",
    "        ssp5_full = pd.concat([self.hist_common, self.ssp5_common])\n",
    "        ssp5_full.index.names = ['TIMESTAMP']\n",
    "\n",
    "        return ssp2_full, ssp5_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af97f4-cae3-4164-8824-843e85bc93f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Usage example\n",
    "processor = CMIPProcessor(dir=cmip_dir, var='pr')\n",
    "ssp2_pr, ssp5_pr = processor.get_results()\n",
    "processor = CMIPProcessor(dir=cmip_dir, var='tas')\n",
    "ssp2_tas, ssp5_tas = processor.get_results()\n",
    "\n",
    "print(ssp2_tas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
