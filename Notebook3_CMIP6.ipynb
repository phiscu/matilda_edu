{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1d1c1b-5071-4074-bfda-619c3d25e1e0",
   "metadata": {},
   "source": [
    "# Climate scenario data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf882979-3ab9-43cd-b846-65744bc79966",
   "metadata": {},
   "source": [
    "In this notebook we will...\n",
    "1. ... aggregate and download climate scenario data from the Coupled Model Intercomparison Project Phase 6 ([CMIP6](https://wcrp-cmip.org/cmip-phase-6-cmip6/)) for our catchment,\n",
    "2. ... preprocess the data,\n",
    "3. ... compare the CMIP6 models with our reanalysis data and adjust them for bias,\n",
    "4. ... and visualize the data before and after bias adjustment.\n",
    "\n",
    "The [NEX-GDDP-CMIP6 dataset](https://www.nature.com/articles/s41597-022-01393-4) we are going to use has been downscaled to 27830 m resolution by the [NASA Climate Analytics Group](https://www.nature.com/articles/s41597-022-01393-4) and is available in two [Shared Socio-Economic Pathways](https://unfccc.int/sites/default/files/part1_iiasa_rogelj_ssp_poster.pdf) (SSP2 and SSP5). It is available via [Google Earth Engine](https://developers.google.com/earth-engine/datasets/catalog/NASA_GDDP-CMIP6#bands) which makes it subsettable on the server side and the download relatively light-weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a812fa8-dff7-4956-91c0-928ac896bfe6",
   "metadata": {},
   "source": [
    "We start by importing and initializing the Google Earth Engine packages again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8866e2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c276e-e094-4010-aa76-a3374c2caf30",
   "metadata": {},
   "source": [
    "The next cell reads the output directory location and the catchment outline as target polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4f7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import ast\n",
    "import geopandas as gpd\n",
    "\n",
    "# read local config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get paths from config.ini\n",
    "dir_output = config['FILE_SETTINGS']['DIR_OUTPUT']\n",
    "dir_figures = config['FILE_SETTINGS']['DIR_FIGURES']\n",
    "output_gpkg = dir_output + config['FILE_SETTINGS']['GPKG_NAME']\n",
    "\n",
    "# get style for matplotlib plots\n",
    "plt_style = ast.literal_eval(config['CONFIG']['PLOT_STYLE'])\n",
    "\n",
    "# set the file format for storage\n",
    "compact_files = config.getboolean('CONFIG','COMPACT_FILES')\n",
    "\n",
    "# load catchment outline as target polygon\n",
    "catchment_new = gpd.read_file(output_gpkg, layer='catchment_new')\n",
    "catchment = geemap.geopandas_to_ee(catchment_new)\n",
    "\n",
    "# name target subdirectory to be created\n",
    "cmip_dir = dir_output + 'cmip6/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6ed62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Select, aggregate, and download downscaled CMIP6 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48377c",
   "metadata": {},
   "source": [
    "We are going to create a class that does it all in one go. The `buildFeauture()` function requests daily catchment-wide averages of all available CMIP6 models for individual years. All years requested are stored in an `ee.ImageCollection` by `getResult()`. To provide the best basis for bias adjustment a large overlap of reanalysis and scenario data is recommended. By default, the `CMIPDownloader` class requests everything between the earliest available date from ERA5-Land (1979) and the latest available date from CMIP6 (2100). The `download()` function then starts a given number of parallel requests, each downloading a single year and storing them in CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41cf5a4-bc87-4fd7-a7a0-92dae7645d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import requests\n",
    "from retry import retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CMIPDownloader:\n",
    "    \"\"\"Class to download spatially averaged CMIP6 data for a given period, variable, and spatial subset.\"\"\"\n",
    "\n",
    "    def __init__(self, var, starty, endy, shape, processes=10, dir='./'):\n",
    "        self.var = var\n",
    "        self.starty = starty\n",
    "        self.endy = endy\n",
    "        self.shape = shape\n",
    "        self.processes = processes\n",
    "        self.directory = dir\n",
    "\n",
    "        # create the download directory if it doesn't exist\n",
    "        if not os.path.exists(self.directory):\n",
    "            os.makedirs(self.directory)\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Runs a subset routine for CMIP6 data on GEE servers to create ee.FeatureCollections for all years in\n",
    "        the requested period. Downloads individual years in parallel processes to increase the download time.\"\"\"\n",
    "        \n",
    "        print('Initiating download request for NEX-GDDP-CMIP6 data from ' +\n",
    "              str(self.starty) + ' to ' + str(self.endy) + '.')\n",
    "\n",
    "        def getRequests(starty, endy):\n",
    "            \"\"\"Generates a list of years to be downloaded. [Client side]\"\"\"\n",
    "\n",
    "            return [i for i in range(starty, endy+1)]\n",
    "\n",
    "        @retry(tries=10, delay=1, backoff=2)\n",
    "        def getResult(index, year):\n",
    "            \"\"\"Handle the HTTP requests to download one year of CMIP6 data. [Server side]\"\"\"\n",
    "\n",
    "            start = str(year) + '-01-01'\n",
    "            end = str(year + 1) + '-01-01'\n",
    "            startDate = ee.Date(start)\n",
    "            endDate = ee.Date(end)\n",
    "            n = endDate.difference(startDate, 'day').subtract(1)\n",
    "\n",
    "            def getImageCollection(var):\n",
    "                \"\"\"Create and image collection of CMIP6 data for the requested variable, period, and region.\n",
    "                [Server side]\"\"\"\n",
    "\n",
    "                collection = ee.ImageCollection('NASA/GDDP-CMIP6') \\\n",
    "                    .select(var) \\\n",
    "                    .filterDate(startDate, endDate) \\\n",
    "                    .filterBounds(self.shape)\n",
    "                return collection\n",
    "\n",
    "            def renameBandName(b):\n",
    "                \"\"\"Edit variable names for better readability. [Server side]\"\"\"\n",
    "\n",
    "                split = ee.String(b).split('_')\n",
    "                return ee.String(split.splice(split.length().subtract(2), 1).join(\"_\"))\n",
    "\n",
    "            def buildFeature(i):\n",
    "                \"\"\"Create an area weighted average of the defined region for every day in the given year.\n",
    "                [Server side]\"\"\"\n",
    "\n",
    "                t1 = startDate.advance(i, 'day')\n",
    "                t2 = t1.advance(1, 'day')\n",
    "                # feature = ee.Feature(point)\n",
    "                dailyColl = collection.filterDate(t1, t2)\n",
    "                dailyImg = dailyColl.toBands()\n",
    "                # renaming and handling names\n",
    "                bands = dailyImg.bandNames()\n",
    "                renamed = bands.map(renameBandName)\n",
    "                # Daily extraction and adding time information\n",
    "                dict = dailyImg.rename(renamed).reduceRegion(\n",
    "                    reducer=ee.Reducer.mean(),\n",
    "                    geometry=self.shape,\n",
    "                ).combine(\n",
    "                    ee.Dictionary({'system:time_start': t1.millis(), 'isodate': t1.format('YYYY-MM-dd')})\n",
    "                )\n",
    "                return ee.Feature(None, dict)\n",
    "\n",
    "            # Create features for all days in the respective year. [Server side]\n",
    "            collection = getImageCollection(self.var)\n",
    "            year_feature = ee.FeatureCollection(ee.List.sequence(0, n).map(buildFeature))\n",
    "\n",
    "            # Create a download URL for a CSV containing the feature collection. [Server side]\n",
    "            url = year_feature.getDownloadURL()\n",
    "\n",
    "            # Handle downloading the actual csv for one year. [Client side]\n",
    "            r = requests.get(url, stream=True)\n",
    "            if r.status_code != 200:\n",
    "                r.raise_for_status()\n",
    "            filename = os.path.join(self.directory, 'cmip6_' + self.var + '_' + str(year) + '.csv')\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(r.text)\n",
    "\n",
    "            return index\n",
    "\n",
    "        # Create a list of years to be downloaded. [Client side]\n",
    "        items = getRequests(self.starty, self.endy)\n",
    "\n",
    "        # Launch download requests in parallel processes and display a status bar. [Client side]\n",
    "        with tqdm(total=len(items), desc=\"Downloading CMIP6 data for variable '\" + self.var + \"'\") as pbar:\n",
    "            results = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.processes) as executor:\n",
    "                for i, year in enumerate(items):\n",
    "                    results.append(executor.submit(getResult, i, year))\n",
    "                for future in concurrent.futures.as_completed(results):\n",
    "                    index = future.result()\n",
    "                    pbar.update(1)\n",
    "\n",
    "        print(\"All downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75591f-5d00-4cad-b067-21c17ed0d7d4",
   "metadata": {},
   "source": [
    "We can now define a target location and start the download for both desired variables individually. We choose a moderate number of requests to avoid kernel \"hickups\". The download time depends on the number of parallel processes, the traffic on GEE servers, and other mysterious factors. If you run this notebook in a Binder it usually doesn't take more than 5 min for both downloads to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f173066-112a-4ce2-bba6-f614d923abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader_t = CMIPDownloader(var='tas', starty=1979, endy=2100, shape=catchment, processes=30, dir=cmip_dir)\n",
    "downloader_t.download()\n",
    "downloader_p = CMIPDownloader(var='pr', starty=1979, endy=2100, shape=catchment, processes=30, dir=cmip_dir)\n",
    "downloader_p.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5dc796-c966-4573-afef-a6c9e8b7d4b3",
   "metadata": {},
   "source": [
    "We have now downloaded individual files for each year and variable and stored them in `cmip_dir`. To use them as model forcing data, they need to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b1f8f-8525-460e-95a7-dbf3d90e609d",
   "metadata": {},
   "source": [
    "## Process the downloaded CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71793100-a5d7-4001-99cf-f8e7d4caa968",
   "metadata": {},
   "source": [
    "The following class will read all downloaded CSV files and concatenate them to a single file per scenario. It further checks for consistency and drops models not available for individual years or scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d5221d-a675-427f-a3a9-890a62e390e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CMIPProcessor:\n",
    "    \"\"\"Class to read and pre-process CSV files downloaded by the CMIPDownloader class.\"\"\"\n",
    "    def __init__(self, var, file_dir='.'):\n",
    "        self.file_dir = file_dir\n",
    "        self.var = var\n",
    "        self.df_hist = self.append_df(self.var, self.file_dir, hist=True)\n",
    "        self.df_ssp = self.append_df(self.var, self.file_dir, hist=False)\n",
    "        self.ssp2_common, self.ssp5_common, self.hist_common,\\\n",
    "            self.common_models, self.dropped_models = self.process_dataframes()\n",
    "        self.ssp2, self.ssp5 = self.get_results()\n",
    "\n",
    "    def read_cmip(self, filename):\n",
    "        \"\"\"Reads CMIP6 CSV files and drops redundant columns.\"\"\"\n",
    "\n",
    "        df = pd.read_csv(filename, index_col='isodate', parse_dates=['isodate'])\n",
    "        df = df.drop(['system:index', '.geo', 'system:time_start'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def append_df(self, var, file_dir='.', hist=True):\n",
    "        \"\"\"Reads CMIP6 CSV files of individual years and concatenates them into dataframes for the full downloaded\n",
    "        period. Historical and scenario datasets are treated separately. Converts precipitation unit to mm.\"\"\"\n",
    "\n",
    "        df_list = []\n",
    "        if hist:\n",
    "            starty = 1979\n",
    "            endy = 2014\n",
    "        else:\n",
    "            starty = 2015\n",
    "            endy = 2100\n",
    "        for i in range(starty, endy + 1):\n",
    "            filename = file_dir + 'cmip6_' + var + '_' + str(i) + '.csv'\n",
    "            df_list.append(self.read_cmip(filename))\n",
    "        if hist:\n",
    "            hist_df = pd.concat(df_list)\n",
    "            if var == 'pr':\n",
    "                hist_df = hist_df * 86400       # from kg/(m^2*s) to mm/day\n",
    "            return hist_df\n",
    "        else:\n",
    "            ssp_df = pd.concat(df_list)\n",
    "            if var == 'pr':\n",
    "                ssp_df = ssp_df * 86400       # from kg/(m^2*s) to mm/day\n",
    "            return ssp_df\n",
    "\n",
    "    def process_dataframes(self):\n",
    "        \"\"\"Separates the two scenarios and drops models not available for both scenarios and the historical period.\"\"\"\n",
    "\n",
    "        ssp2 = self.df_ssp.loc[:, self.df_ssp.columns.str.startswith('ssp245')]\n",
    "        ssp5 = self.df_ssp.loc[:, self.df_ssp.columns.str.startswith('ssp585')]\n",
    "        hist = self.df_hist.loc[:, self.df_hist.columns.str.startswith('historical')]\n",
    "\n",
    "        ssp2.columns = ssp2.columns.str.lstrip('ssp245_').str.rstrip('_' + self.var)\n",
    "        ssp5.columns = ssp5.columns.str.lstrip('ssp585_').str.rstrip('_' + self.var)\n",
    "        hist.columns = hist.columns.str.lstrip('historical_').str.rstrip('_' + self.var)\n",
    "\n",
    "        # Get all the models the three datasets have in common\n",
    "        common_models = set(ssp2.columns).intersection(ssp5.columns).intersection(hist.columns)\n",
    "\n",
    "        # Get the model names that contain NaN values\n",
    "        nan_models_list = [df.columns[df.isna().any()].tolist() for df in [ssp2, ssp5, hist]]\n",
    "        # flatten the list\n",
    "        nan_models = [col for sublist in nan_models_list for col in sublist]\n",
    "        # remove duplicates\n",
    "        nan_models = list(set(nan_models))\n",
    "\n",
    "        # Remove models with NaN values from the list of common models\n",
    "        common_models = [x for x in common_models if x not in nan_models]\n",
    "\n",
    "        ssp2_common = ssp2.loc[:, common_models]\n",
    "        ssp5_common = ssp5.loc[:, common_models]\n",
    "        hist_common = hist.loc[:, common_models]\n",
    "\n",
    "        dropped_models = list(set([mod for mod in ssp2.columns if mod not in common_models] +\n",
    "                                  [mod for mod in ssp5.columns if mod not in common_models] +\n",
    "                                  [mod for mod in hist.columns if mod not in common_models]))\n",
    "\n",
    "        return ssp2_common, ssp5_common, hist_common, common_models, dropped_models\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Concatenates historical and scenario data to combined dataframes of the full downloaded period.\n",
    "        Arranges the models in alphabetical order.\"\"\"\n",
    "\n",
    "        ssp2_full = pd.concat([self.hist_common, self.ssp2_common])\n",
    "        ssp2_full.index.names = ['TIMESTAMP']\n",
    "        ssp5_full = pd.concat([self.hist_common, self.ssp5_common])\n",
    "        ssp5_full.index.names = ['TIMESTAMP']\n",
    "\n",
    "        ssp2_full = ssp2_full.reindex(sorted(ssp2_full.columns), axis=1)\n",
    "        ssp5_full = ssp5_full.reindex(sorted(ssp5_full.columns), axis=1)\n",
    "\n",
    "\n",
    "        return ssp2_full, ssp5_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bbe1e-26d7-4857-b9d9-bd511ce8d593",
   "metadata": {},
   "source": [
    "The `CMIPProcessor` processes variables individually and returns a single dataframe for each of both scenarios from 1979 to 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a803fbe3-29d0-4129-89d5-b1af7d56e18c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmip_dir = dir_output + 'cmip6/'\n",
    "\n",
    "processor_t = CMIPProcessor(file_dir=cmip_dir, var='tas')\n",
    "ssp2_tas_raw, ssp5_tas_raw = processor_t.get_results()\n",
    "\n",
    "processor_p = CMIPProcessor(file_dir=cmip_dir, var='pr')\n",
    "ssp2_pr_raw, ssp5_pr_raw = processor_p.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8d659-bc7d-4bb1-85c4-51449269456c",
   "metadata": {},
   "source": [
    "Let's have a look. We can see that our scenario dataset now contains 33 CMIP6 models in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f828625e-0379-430c-9851-1c0450a19e34",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "print(ssp2_tas_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2445170-2b45-45ed-bfb7-3d55ff2529c7",
   "metadata": {},
   "source": [
    "If we want to check which models failed the consistency check of the `CMIPProcessor` we can use its `dropped_models` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cf9d10-2700-42de-8a7d-72a8cf23bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor_t.dropped_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f769cff-1680-4276-ad42-8c431355d02c",
   "metadata": {},
   "source": [
    "## Bias adjustment using reananlysis data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99297b-9922-465a-94ff-3f4d6af95edb",
   "metadata": {},
   "source": [
    "Due to the coarse resolution of global climate models (GCMs) and the extensive correction of reanalysis data there is substantial bias between the two datasets. To force a glacio-hydrological model calibrated on reanalysis data with climate scenarios this bias needs to be adressed. We will use a method developed by [Switanek et.al. (2017)](https://doi.org/10.5194/hess-21-2649-2017) called Scaled Distribution Mapping (SDM) to correct for bias while preserving trends and the likelihood of meteorological events in the raw GCM data. The method has been implemented in the [`bias_correction`](https://github.com/pankajkarman/bias_correction) Python library by [Pankaj Kumar](https://pankajkarman.github.io/).\n",
    "We will first create a function to read our reanalysis CSV. The `adjust_bias()` function will then loop over all models and adjust them to the reanalysis data in the overlap period (1979 to 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e5c854-38c8-4c6b-a2a4-cf298abdb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bias_correction import BiasCorrection\n",
    "\n",
    "def read_era5l(file):\n",
    "    \"\"\"Reads ERA5-Land data, drops redundant columns, and adds DatetimeIndex.\n",
    "    Resamples the dataframe to reduce the DatetimeIndex to daily resolution.\"\"\"\n",
    "    \n",
    "    return pd.read_csv(file, **{\n",
    "        'usecols':      ['temp', 'prec', 'dt'],\n",
    "        'index_col':    'dt',\n",
    "        'parse_dates':  ['dt']}).resample('D').agg({'temp': 'mean', 'prec': 'sum'})\n",
    "\n",
    "def adjust_bias(predictand, predictor, method='normal_mapping'):\n",
    "    \"\"\"Applies scaled distribution mapping to all passed climate projections (predictand)\n",
    "     based on a predictor timeseries.\"\"\"\n",
    "    \n",
    "    predictor = read_era5l(predictor)\n",
    "    if predictand.mean().mean() > 100:\n",
    "        var = 'temp'\n",
    "    else:\n",
    "        var = 'prec'\n",
    "    training_period = slice('1979-01-01', '2022-12-31')\n",
    "    prediction_period = slice('1979-01-01', '2100-12-31')\n",
    "    corr = pd.DataFrame()\n",
    "    for m in predictand.columns:\n",
    "        x_train = predictand[m][training_period].squeeze()\n",
    "        y_train = predictor[training_period][var].squeeze()\n",
    "        x_predict = predictand[m][prediction_period].squeeze()\n",
    "        bc_corr = BiasCorrection(y_train, x_train, x_predict)\n",
    "        corr[m] = pd.DataFrame(bc_corr.correct(method=method))\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3458c5-3bbc-4c2f-a604-d45192b8cfff",
   "metadata": {},
   "source": [
    "The function is applied to every variable and scenario separately. The `bias_adjustment` library offers a normal and a gamma distribution as basis for the SDM. As the distribution of the ERA5-Land precipitation data is actually closer to normal distribution capped at 0 mm, we use the `normal_mapping` method for both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0cd56c2-a4e0-46a0-bb01-575451c67aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_file = dir_output + 'ERA5L.csv'\n",
    "\n",
    "ssp2_tas = adjust_bias(predictand=ssp2_tas_raw, predictor=era5_file)\n",
    "ssp5_tas = adjust_bias(predictand=ssp5_tas_raw, predictor=era5_file)\n",
    "ssp2_pr = adjust_bias(predictand=ssp2_pr_raw, predictor=era5_file)\n",
    "ssp5_pr = adjust_bias(predictand=ssp5_pr_raw, predictor=era5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1b799-a3da-4cd9-9776-06439a09a72a",
   "metadata": {},
   "source": [
    "The result is a comprehensive dataset of 33 models over 122 years in two versions (pre- and post-adjustment) for every variable. To see what's in the data and what happened during bias adjustment we need an overview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7cf16-b55e-4320-892c-149a6ff5ddde",
   "metadata": {},
   "source": [
    "## \tVisualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ff437-4dda-4295-88dc-6969591b89e1",
   "metadata": {},
   "source": [
    "First, we store our raw and adjusted data in dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a09cee1-db3f-4aeb-a47f-fb7f467d47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_tas_dict = {'SSP2_raw': ssp2_tas_raw, 'SSP2_adjusted': ssp2_tas, 'SSP5_raw': ssp5_tas_raw, 'SSP5_adjusted': ssp5_tas}\n",
    "ssp_pr_dict = {'SSP2_raw': ssp2_pr_raw, 'SSP2_adjusted': ssp2_pr, 'SSP5_raw': ssp5_pr_raw, 'SSP5_adjusted': ssp5_pr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5de87-0a0c-4fc5-a360-96cf18d531dd",
   "metadata": {},
   "source": [
    "The first plot will contain simple timeseries. The first function `cmip_plot()` resamples the data so a given frequency and creates a single plot. `cmip_plot_combined()` arranges multiple plots for both scenarios before and after bias adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19d590-7a50-4b2b-a5b6-e5409db47edf",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "984e05e5-30e9-443f-a9f3-f3ccb51fcd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "# set style from config\n",
    "plt.style.use(plt_style)\n",
    "\n",
    "def cmip_plot(ax, df, target, title=None, precip=False, intv_sum='ME', intv_mean='10YE',\n",
    "              target_label='Target', show_target_label=False):\n",
    "    \"\"\"Resamples and plots climate model and target data.\"\"\"\n",
    "\n",
    "    if intv_mean == '10YE' or intv_mean == '5YE' or intv_mean == '20YE':\n",
    "        closure = 'left'\n",
    "    else:\n",
    "        closure = 'right'\n",
    "\n",
    "    if not precip:\n",
    "        ax.plot(df.resample(intv_mean, closed=closure, label='left').mean().iloc[:, :], linewidth=1.2)\n",
    "        era_plot, = ax.plot(target['temp'].resample(intv_mean).mean(), linewidth=1.5, c='red', label=target_label,\n",
    "                            linestyle='dashed')\n",
    "    else:\n",
    "        ax.plot(df.resample(intv_sum).sum().resample(intv_mean, closed=closure, label='left').mean().iloc[:, :],\n",
    "                linewidth=1.2)\n",
    "        era_plot, = ax.plot(target['prec'].resample(intv_sum).sum().resample(intv_mean).mean(), linewidth=1.5,\n",
    "                            c='red', label=target_label, linestyle='dashed')\n",
    "    if show_target_label:\n",
    "        ax.legend(handles=[era_plot], loc='upper left')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    \n",
    "\n",
    "def cmip_plot_combined(data, target, title=None, precip=False, intv_sum='ME', intv_mean='10YE',\n",
    "                       target_label='Target', show=False, saveas=None):\n",
    "    \"\"\"Combines multiple subplots of climate data in different scenarios before and after bias adjustment.\n",
    "    Shows target data for comparison\"\"\"\n",
    "\n",
    "    figure, axis = plt.subplots(2, 2, figsize=(12, 12), sharex=\"col\", sharey=\"all\")\n",
    "    \n",
    "    t_kwargs = {'target': target, 'intv_mean': intv_mean, 'target_label': target_label}\n",
    "    p_kwargs = {'target': target, 'intv_mean': intv_mean, 'target_label': target_label,\n",
    "                'intv_sum': intv_sum, 'precip': True}\n",
    "\n",
    "    if not precip:\n",
    "        cmip_plot(axis[0, 0], data['SSP2_raw'], show_target_label=True, title='SSP2 raw', **t_kwargs)\n",
    "        cmip_plot(axis[0, 1], data['SSP2_adjusted'], title='SSP2 adjusted', **t_kwargs)\n",
    "        cmip_plot(axis[1, 0], data['SSP5_raw'], title='SSP5 raw', **t_kwargs)\n",
    "        cmip_plot(axis[1, 1], data['SSP5_adjusted'], title='SSP5 adjusted', **t_kwargs)\n",
    "        figure.legend(data['SSP5_adjusted'].columns, loc='lower right', ncol=6, mode=\"expand\")\n",
    "        figure.tight_layout()\n",
    "        figure.subplots_adjust(bottom=0.15, top=0.92)\n",
    "        figure.suptitle(title, fontweight='bold')\n",
    "        if saveas is not None:\n",
    "            plt.savefig(dir_figures + saveas)\n",
    "        if show:\n",
    "            plt.show()\n",
    "    else:\n",
    "        cmip_plot(axis[0, 0], data['SSP2_raw'], show_target_label=True, title='SSP2 raw', **p_kwargs)\n",
    "        cmip_plot(axis[0, 1], data['SSP2_adjusted'], title='SSP2 adjusted', **p_kwargs)\n",
    "        cmip_plot(axis[1, 0], data['SSP5_raw'], title='SSP5 raw', **p_kwargs)\n",
    "        cmip_plot(axis[1, 1], data['SSP5_adjusted'], title='SSP5 adjusted', **p_kwargs)\n",
    "        figure.legend(data['SSP5_adjusted'].columns, loc='lower right', ncol=6, mode=\"expand\")\n",
    "        figure.tight_layout()\n",
    "        figure.subplots_adjust(bottom=0.15, top=0.92)\n",
    "        figure.suptitle(title, fontweight='bold')\n",
    "        if saveas is not None:\n",
    "            plt.savefig(dir_figures + saveas)\n",
    "        if show:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c3006-4456-4672-a1d5-78b922afb61a",
   "metadata": {},
   "source": [
    "By default temperature data is resampled to 10y means (`intv_mean='10YE'`), precipitation data is shown in 10y (`intv_mean='10YE'`) means of monthly sums (`intv_sum='ME'`). You can adapt this as you please by specifying the respective arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81b0c49-0d51-4b89-a0b3-1df7e8a9cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = read_era5l(era5_file)\n",
    "\n",
    "cmip_plot_combined(data=ssp_tas_dict, target=era5, title='10y Mean of Air Temperature', target_label='ERA5-Land', show=True, saveas='NB3_CMIP6_Temp.png')\n",
    "cmip_plot_combined(data=ssp_pr_dict, target=era5, title='10y Mean of Monthly Precipitation', precip=True, target_label='ERA5-Land', show=True, intv_mean='10YE', saveas='NB3_CMIP6_Prec.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996789a-e78c-4436-8e35-5d79f7d5fd0f",
   "metadata": {},
   "source": [
    "Apparently, some models have striking curves indicating unrealistic outliers or sudden jumps in the data. To clearly identify faulty time series, one option is to choose a qualitative approach by identifying the models using an interactive `plotly` plot. Here we can zoom and select/deselect curves as we like, to identify model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f484fa5-fafe-4067-acec-05c4d6169d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(ssp5_tas_raw.resample('10YE', closed='left', label='left').mean())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93828e-7d7a-47f2-99aa-4262afab1f8d",
   "metadata": {},
   "source": [
    "### Violin plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c837b38-5186-4912-a8a0-b58855be868c",
   "metadata": {},
   "source": [
    "To look at it from a different perpective we can also have a look at the individual distributions of all models. A nice way to cover several aspects at once is to use `seaborne` [violinplots](https://seaborn.pydata.org/generated/seaborn.violinplot.html).\n",
    "\n",
    "First we have to rearrange our input dictionaries a little bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bace284c-3dd8-4768-9de0-4f6a63e12837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_filter(dictionary, filter_string):\n",
    "    \"\"\"Returns a dict with all elements of the input dict that contain a filter string in their keys.\"\"\"\n",
    "    return {key.split('_')[0]: value for key, value in dictionary.items() if filter_string in key}\n",
    "\n",
    "\n",
    "tas_raw = dict_filter(ssp_tas_dict, 'raw')\n",
    "tas_adjusted = dict_filter(ssp_tas_dict, 'adjusted')\n",
    "pr_raw = dict_filter(ssp_pr_dict, 'raw')\n",
    "pr_adjusted = dict_filter(ssp_pr_dict, 'adjusted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d371ed-b900-4200-a4b0-eb705554aa55",
   "metadata": {},
   "source": [
    "The `violinplot()` function requires input data in `long` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2bf776-b26c-4bd0-aef5-c1837cbbea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2long(df, intv_sum='ME', intv_mean='YE', precip=False):\n",
    "    \"\"\"Resamples dataframes and converts them into long format to be passed to seaborn.lineplot().\"\"\"\n",
    "\n",
    "    if precip:\n",
    "        df = df.resample(intv_sum).sum().resample(intv_mean).mean()\n",
    "        df = df.reset_index()\n",
    "        df = df.melt('TIMESTAMP', var_name='model', value_name='prec')\n",
    "    else:\n",
    "        df = df.resample(intv_mean).mean()\n",
    "        df = df.reset_index()\n",
    "        df = df.melt('TIMESTAMP', var_name='model', value_name='temp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e8ea5-da53-4983-8eee-cf6ce61300ae",
   "metadata": {},
   "source": [
    "For comparison the `vplots()` function will arrange the plots in a similar grid as in the figures above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8817ffe0-a1c5-4ae0-9600-031c853401f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vplots(before, after, target, target_label='Target', precip=False, show=False, saveas=None):\n",
    "    \"\"\"Creates violin plots of the kernel density estimation for all models before and after bias adjustment.\"\"\"\n",
    "\n",
    "    period = slice('1979-01-01', '2022-12-31')\n",
    "    if precip:\n",
    "        var = 'prec'\n",
    "        var_label = 'Annual Precipitation'\n",
    "        unit = ' [mm]'\n",
    "    else:\n",
    "        var = 'temp'\n",
    "        var_label = 'Mean Annual Air Temperature'\n",
    "        unit = ' [K]'\n",
    "    for i in before.keys():\n",
    "        before[i] = before[i].loc[period].copy()\n",
    "        before[i][target_label] = target[var][period]\n",
    "\n",
    "    for i in after.keys():\n",
    "        after[i] = after[i].loc[period].copy()\n",
    "        after[i][target_label] = target[var][period]\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    outer = fig.add_gridspec(1, 2)\n",
    "    inner = outer[0].subgridspec(2, 1)\n",
    "    axis = inner.subplots(sharex='col')\n",
    "\n",
    "    all_data = pd.concat([df2long(before[i], precip=precip, intv_sum='YE') for i in before.keys()] +\n",
    "                         [df2long(after[i], precip=precip, intv_sum='YE') for i in after.keys()])\n",
    "    xmin, xmax = all_data[var].min(), all_data[var].max()\n",
    "\n",
    "    if precip:\n",
    "        xlim = (xmin * 0.95, xmax * 1.05)\n",
    "    else:\n",
    "        xlim = (xmin - 1, xmax +1 )\n",
    "\n",
    "    for (i, k) in zip(before.keys(), range(0, 4, 1)):\n",
    "        df = df2long(before[i], precip=precip, intv_sum='YE')\n",
    "        axis[k].grid()\n",
    "        sns.violinplot(ax=axis[k], x=var, y='model', data=df, density_norm=\"count\", bw_adjust=.2)\n",
    "        axis[k].set(xlim=xlim)\n",
    "        axis[k].set_ylabel(i, fontsize=18, fontweight='bold')\n",
    "        if k == 0:\n",
    "            axis[k].set_title('Before Scaled Distribution Mapping', fontweight='bold', fontsize=18)\n",
    "    plt.xlabel(var_label + unit)\n",
    "\n",
    "    inner = outer[1].subgridspec(2, 1)\n",
    "    axis = inner.subplots(sharex='col')\n",
    "    for (i, k) in zip(after.keys(), range(0, 4, 1)):\n",
    "        df = df2long(after[i], precip=precip, intv_sum='YE')\n",
    "        axis[k].grid()\n",
    "        sns.violinplot(ax=axis[k], x=var, y='model', data=df, density_norm=\"count\", bw_adjust=.2)\n",
    "        axis[k].set(xlim=xlim)\n",
    "        axis[k].set_ylabel(i, fontsize=18, fontweight='bold')\n",
    "        axis[k].get_yaxis().set_visible(False)\n",
    "        if k == 0:\n",
    "            axis[k].set_title('After Scaled Distribution Mapping', fontweight='bold', fontsize=18)\n",
    "    plt.xlabel(var_label + unit)\n",
    "\n",
    "    starty = period.start.split('-')[0]\n",
    "    endy = period.stop.split('-')[0]\n",
    "    fig.suptitle('Kernel Density Estimation of ' + var_label + ' (' + starty + '-' + endy + ')',\n",
    "                 fontweight='bold', fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.93)\n",
    "    if saveas is not None:\n",
    "        plt.savefig(dir_figures + saveas)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7111e-bc10-429f-9d94-ffd98972a952",
   "metadata": {},
   "source": [
    "Again, we run the plotting function for every variable individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8df5851-0c16-4773-9872-e4071d18a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "vplots(tas_raw, tas_adjusted, era5, target_label='ERA5-Land', show=True, saveas='NB3_vplot_Temp.png')\n",
    "vplots(pr_raw, pr_adjusted, era5, target_label='ERA5-Land', precip=True, show=True, saveas='NB3_vplot_Prec.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b174c7d-407b-448f-9e9c-3c756df8a28e",
   "metadata": {},
   "source": [
    "### Data filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f2a7b-54c5-46e7-94e7-fef149974ffc",
   "metadata": {},
   "source": [
    "Alternatively, we can create some helper functions to remove respective models automatically and combine them in a handy class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b8e119-7cbe-48f8-bdea-77cadf003e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFilter:\n",
    "    def __init__(self, df, zscore_threshold=3, resampling_rate=None, prec=False, jump_threshold=5):\n",
    "        self.df = df\n",
    "        self.zscore_threshold = zscore_threshold\n",
    "        self.resampling_rate = resampling_rate\n",
    "        self.prec = prec\n",
    "        self.jump_threshold = jump_threshold\n",
    "        self.filter_all()\n",
    "\n",
    "\n",
    "    def check_outliers(self):\n",
    "        \"\"\"\n",
    "        A function for filtering a pandas dataframe for columns with obvious outliers\n",
    "        and dropping them based on a z-score threshold.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        models : list\n",
    "            A list of columns identified as having outliers.\n",
    "        \"\"\"\n",
    "        # Resample if rate specified\n",
    "        if self.resampling_rate is not None:\n",
    "            if self.prec:\n",
    "                self.df = self.df.resample(self.resampling_rate).sum()\n",
    "            else:\n",
    "                self.df = self.df.resample(self.resampling_rate).mean()\n",
    "\n",
    "        # Calculate z-scores for each column\n",
    "        z_scores = pd.DataFrame((self.df - self.df.mean()) / self.df.std())\n",
    "\n",
    "        # Identify columns with at least one outlier (|z-score| > threshold)\n",
    "        cols_with_outliers = z_scores.abs().apply(lambda x: any(x > self.zscore_threshold))\n",
    "        self.outliers = list(self.df.columns[cols_with_outliers])\n",
    "\n",
    "        # Return the list of columns with outliers\n",
    "        return self.outliers\n",
    "\n",
    "    def check_jumps(self):\n",
    "        \"\"\"\n",
    "        A function for checking a pandas dataframe for columns with sudden jumps or drops\n",
    "        and returning a list of the columns that have them.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        jumps : list\n",
    "            A list of columns identified as having sudden jumps or drops.\n",
    "        \"\"\"\n",
    "        cols = self.df.columns\n",
    "        jumps = []\n",
    "\n",
    "        for col in cols:\n",
    "            diff = self.df[col].diff()\n",
    "            if (abs(diff) > self.jump_threshold).any():\n",
    "                jumps.append(col)\n",
    "\n",
    "        self.jumps = jumps\n",
    "        return self.jumps\n",
    "\n",
    "    def filter_all(self):\n",
    "        \"\"\"\n",
    "        A function for filtering a dataframe for columns with obvious outliers\n",
    "        or sudden jumps or drops in temperature, and returning a list of the\n",
    "        columns that have been filtered using either or both methods.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_models : list\n",
    "            A list of columns identified as having outliers or sudden jumps/drops in temperature.\n",
    "        \"\"\"\n",
    "        self.check_outliers()\n",
    "        self.check_jumps()\n",
    "        self.filtered_models = list(set(self.outliers) | set(self.jumps))\n",
    "        return self.filtered_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3488e60-6091-4f20-bc96-a48162e48003",
   "metadata": {},
   "source": [
    "By default the `DataFilter` class filters models containing ...\n",
    "\n",
    "  ... outliers deviating more than 3 standard deviations from the mean (`zscore_threshold`) and/or ...\n",
    "\n",
    "  ... increases/decreases of more than 5 units in a single timestep (`jump_threshold`).\n",
    "\n",
    "The functions can be applied separately (`check_outliers` or `check_jumps`) or together (`filter_all`). All three return a `list` of model names.\n",
    "\n",
    "Here, we also use the `resampling_rate` parameter to resample the data to annual means (`'YE'`) before running the checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ccbcfa6-2371-4fa0-a8d3-d0111b5d650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = DataFilter(ssp5_tas_raw, zscore_threshold=3, jump_threshold=5, resampling_rate='YE')\n",
    "\n",
    "print('Models with temperature outliers: ' + str(filter.outliers))\n",
    "print('Models with temperature jumps: ' + str(filter.jumps))\n",
    "print('Models with either one or both: ' + str(filter.filtered_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24341ec-7f30-4660-83b3-7695cb3faafd",
   "metadata": {},
   "source": [
    "The identified columns can then be removed from the CMIP6 ensemble dataset using another helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43debca5-c04a-4089-b9b2-2ff97dcf2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_model(col_names, dict_or_df):\n",
    "    \"\"\"\n",
    "    Drop columns with given names from either a dictionary of dataframes\n",
    "    or a single dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_names : list of str\n",
    "        The list of model names to drop.\n",
    "    dict_or_df : dict of pandas.DataFrame or pandas.DataFrame\n",
    "        If a dict of dataframes, all dataframes in the dict will be edited.\n",
    "        If a single dataframe, only that dataframe will be edited.\n",
    "    Returns\n",
    "    -------\n",
    "    dict_of_dfs : dict of pandas.DataFrame or pandas.DataFrame\n",
    "        The updated dictionary of dataframes or dataframe with dropped columns.\n",
    "    \"\"\"\n",
    "    if isinstance(dict_or_df, dict):\n",
    "        # loop through the dictionary and edit each dataframe\n",
    "        for key in dict_or_df.keys():\n",
    "            if all(col_name in dict_or_df[key].columns for col_name in col_names):\n",
    "                dict_or_df[key] = dict_or_df[key].drop(columns=col_names)\n",
    "        return dict_or_df\n",
    "    elif isinstance(dict_or_df, pd.DataFrame):\n",
    "        # edit the single dataframe\n",
    "        if all(col_name in dict_or_df.columns for col_name in col_names):\n",
    "            return dict_or_df.drop(columns=col_names)\n",
    "    else:\n",
    "        raise TypeError('Input must be a dictionary or a dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc48aa1-54f5-4ec9-8d85-06ddeda8d2f6",
   "metadata": {},
   "source": [
    "We run the `drop_model()` function on the dictionaries of all variables and run `cmip_plot_combined()` again to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ffd8d9f-f092-46c1-b367-9cfaa0526915",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_tas_dict = drop_model(filter.filtered_models, ssp_tas_dict)\n",
    "ssp_pr_dict = drop_model(filter.filtered_models, ssp_pr_dict)\n",
    "\n",
    "\n",
    "cmip_plot_combined(data=ssp_tas_dict, target=era5, title='10y Mean of Air Temperature', target_label='ERA5-Land', show=True, saveas='NB3_CMIP6_Temp_filtered.png')\n",
    "cmip_plot_combined(data=ssp_pr_dict, target=era5, title='10y Mean of Monthly Precipitation', precip=True, target_label='ERA5-Land', show=True, intv_mean='10YE', saveas='NB3_CMIP6_Prec_filtered.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02a599-913e-4c8d-a61e-8f92c14d82c2",
   "metadata": {},
   "source": [
    "### Ensemble mean plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80373455-dc31-4d35-8f01-be8966694d20",
   "metadata": {},
   "source": [
    "As we now don't need to focus on individual models anymore, we can reduce the number of lines by only plotting the ensemble means with a 90% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "435b8a6b-a48f-47cc-a715-705f04975e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "\n",
    "def cmip_plot_ensemble(cmip, target, precip=False, intv_sum='ME', intv_mean='YE', figsize=(10, 6), show=True, saveas=None):\n",
    "    \"\"\"\n",
    "    Plots the multi-model mean of climate scenarios including the 90% confidence interval.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmip: dict\n",
    "        A dictionary with keys representing the different CMIP6 models and scenarios as pandas dataframes\n",
    "        containing data of temperature and/or precipitation.\n",
    "    target: pandas.DataFrame\n",
    "        Dataframe containing the historical reanalysis data.\n",
    "    precip: bool\n",
    "        If True, plot the mean precipitation. If False, plot the mean temperature. Default is False.\n",
    "    intv_sum: str\n",
    "        Interval for precipitation sums. Default is monthly ('ME').\n",
    "    intv_mean: str\n",
    "        Interval for the mean of temperature data or precipitation sums. Default is annual ('YE').\n",
    "    figsize: tuple\n",
    "        Figure size for the plot. Default is (10,6).\n",
    "    show: bool\n",
    "        If True, show the resulting plot. If False, do not show it. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "    figure, axis = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Define color palette\n",
    "    colors = ['darkorange', 'orange', 'darkblue', 'dodgerblue']\n",
    "    # create a new dictionary with the same keys but new values from the list\n",
    "    col_dict = {key: value for key, value in zip(cmip.keys(), colors)}\n",
    "\n",
    "    if precip:\n",
    "        for i in cmip.keys():\n",
    "            df = df2long(cmip[i], intv_sum=intv_sum, intv_mean=intv_mean, precip=True)\n",
    "            sns.lineplot(data=df, x='TIMESTAMP', y='prec', color=col_dict[i])\n",
    "        axis.set(xlabel='Year', ylabel='Mean Precipitation [mm]')\n",
    "        if intv_sum=='M':\n",
    "            figure.suptitle('Mean Monthly Precipitation', fontweight='bold')\n",
    "        elif intv_sum=='Y':\n",
    "            figure.suptitle('Mean Annual Precipitation', fontweight='bold')\n",
    "        target_plot = axis.plot(target.resample(intv_sum).sum().resample(intv_mean).mean(), linewidth=1.5, c='black',\n",
    "                             label='ERA5', linestyle='dashed')\n",
    "    else:\n",
    "        for i in cmip.keys():\n",
    "            df = df2long(cmip[i], intv_mean=intv_mean)\n",
    "            sns.lineplot(data=df, x='TIMESTAMP', y='temp', color=col_dict[i])\n",
    "        axis.set(xlabel='Year', ylabel='Mean Air Temperature [K]')\n",
    "        if intv_mean=='10Y':\n",
    "            figure.suptitle('Mean 10y Air Temperature', fontweight='bold')\n",
    "        elif intv_mean == 'Y':\n",
    "            figure.suptitle('Mean Annual Air Temperature', fontweight='bold')\n",
    "        elif intv_mean == 'M':\n",
    "            figure.suptitle('Mean Monthly Air Temperature', fontweight='bold')\n",
    "        target_plot = axis.plot(target.resample(intv_mean).mean(), linewidth=1.5, c='black',\n",
    "                         label='ERA5', linestyle='dashed')\n",
    "    axis.legend(['SSP2_raw', '_ci1', 'SSP2_adjusted', '_ci2', 'SSP5_raw', '_ci3', 'SSP5_adjusted', '_ci4'],\n",
    "                loc=\"upper center\", bbox_to_anchor=(0.43, -0.15), ncol=4,\n",
    "                frameon=False)  # First legend --> Workaround as seaborn lists CIs in legend\n",
    "    leg = Legend(axis, target_plot, ['ERA5L'], loc='upper center', bbox_to_anchor=(0.83, -0.15), ncol=1,\n",
    "                 frameon=False)  # Second legend (ERA5)\n",
    "    axis.add_artist(leg)\n",
    "    plt.grid()\n",
    "\n",
    "    figure.tight_layout(rect=[0, 0.02, 1, 1])  # Make some room at the bottom\n",
    "\n",
    "    if saveas is not None:\n",
    "        plt.savefig(dir_figures+saveas)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    warnings.filterwarnings(action='always')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82a1f9-ecde-4e4e-980f-06b53136a398",
   "metadata": {},
   "source": [
    "With less lines in the plot, we can also reduce the resample frequency and show annual means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc806530-2b13-4a61-806c-8ecee9f6b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip_plot_ensemble(ssp_tas_dict, era5['temp'], intv_mean='YE', saveas='NB3_CMIP6_Ensemble_Temp.png')\n",
    "cmip_plot_ensemble(ssp_pr_dict, era5['prec'], precip=True, intv_sum='ME', intv_mean='YE', saveas='NB3_CMIP6_Ensemble_Prec.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c0b3e-9312-4093-a5d5-b08757191e7d",
   "metadata": {},
   "source": [
    "We can see that the SDM adjusts the range and mean of the target data while preserving the distribution and trend of the original data. However, the inter-model variance is slightly reduced for temperature and significantly increased for precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ae10b-1b7a-47a9-906a-22ede40937a7",
   "metadata": {},
   "source": [
    "Last but not least, we will have a closer look at the performance of the bias adjustment. To do that, we will create probability plots for all models comparing original, target, and adjusted data with each other and a standard normal distribution. The `prob_plot` function creates such a plot for an individual model and scenario. The `pp_matrix` function loops the `prob_plot` function over all models in a `DataFrame` and arranges them in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b4d8558-19f4-40aa-b67d-01bcebd02e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probscale\n",
    "\n",
    "def prob_plot(original, target, corrected, ax, title=None, ylabel=\"Temperature [K]\", **kwargs):\n",
    "    \"\"\"\n",
    "    Combines probability plots of climate model data before and after bias adjustment\n",
    "    and the target data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original : pandas.DataFrame\n",
    "        The original climate model data.\n",
    "    target : pandas.DataFrame\n",
    "        The target data.\n",
    "    corrected : pandas.DataFrame\n",
    "        The climate model data after bias adjustment.\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes on which to plot the probability plot.\n",
    "    title : str, optional\n",
    "        The title of the plot. Default is None.\n",
    "    ylabel : str, optional\n",
    "        The label for the y-axis. Default is \"Temperature [K]\".\n",
    "    **kwargs : dict, optional\n",
    "        Additional keyword arguments passed to the probscale.probplot() function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig : matplotlib Figure\n",
    "        The generated figure.\n",
    "    \"\"\"\n",
    "\n",
    "    scatter_kws = dict(label=\"\", marker=None, linestyle=\"-\")\n",
    "    common_opts = dict(plottype=\"qq\", problabel=\"\", datalabel=\"\", **kwargs)\n",
    "\n",
    "    scatter_kws[\"label\"] = \"original\"\n",
    "    fig = probscale.probplot(original, ax=ax, scatter_kws=scatter_kws, **common_opts)\n",
    "\n",
    "    scatter_kws[\"label\"] = \"target\"\n",
    "    fig = probscale.probplot(target, ax=ax, scatter_kws=scatter_kws, **common_opts)\n",
    "\n",
    "    scatter_kws[\"label\"] = \"adjusted\"\n",
    "    fig = probscale.probplot(corrected, ax=ax, scatter_kws=scatter_kws, **common_opts)\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_xlabel(\"Standard Normal Quantiles\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True)\n",
    "\n",
    "    score = round(target.corr(corrected), 2)\n",
    "    ax.text(0.05, 0.8, f\"R² = {score}\", transform=ax.transAxes, fontsize=15)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def pp_matrix(original, target, corrected, scenario=None, nrow=7, ncol=5, precip=False, show=False, saveas=None):\n",
    "    \"\"\"\n",
    "    Arranges the prob_plots of all CMIP6 models in a matrix and adds the R² score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original : pandas.DataFrame\n",
    "        The original climate model data.\n",
    "    target : pandas.DataFrame\n",
    "        The target data.\n",
    "    corrected : pandas.DataFrame\n",
    "        The climate model data after bias adjustment.\n",
    "    scenario : str, optional\n",
    "        The climate scenario to be added to the plot title.\n",
    "    nrow : int, optional\n",
    "        The number of rows in the plot matrix. Default is 7.\n",
    "    ncol : int, optional\n",
    "        The number of columns in the plot matrix. Default is 5.\n",
    "    precip : bool, optional\n",
    "        Indicates whether the data is precipitation data. Default is False.\n",
    "    show : bool, optional\n",
    "        Indicates whether to display the plot. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    period = slice('1979-01-01', '2022-12-31')\n",
    "    if precip:\n",
    "        var = 'Precipitation'\n",
    "        var_label = 'Monthly ' + var\n",
    "        unit = ' [mm]'\n",
    "        original = original.resample('ME').sum()\n",
    "        target = target.resample('ME').sum()\n",
    "        corrected = corrected.resample('ME').sum()\n",
    "    else:\n",
    "        var = 'Temperature'\n",
    "        var_label = 'Daily Mean ' + var\n",
    "        unit = ' [K]'\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "    for i, col in enumerate(original.columns):\n",
    "        ax = plt.subplot(nrow, ncol, i + 1)\n",
    "        prob_plot(original[col][period], target[period],\n",
    "                  corrected[col][period], ax=ax, ylabel=var + unit)\n",
    "        ax.set_title(col, fontweight='bold')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, ['original (CMIP6 raw)', 'target (ERA5-Land)', 'adjusted (CMIP6 after SDM)'], loc='lower right',\n",
    "               bbox_to_anchor=(0.96, 0.024), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "    starty = period.start.split('-')[0]\n",
    "    endy = period.stop.split('-')[0]\n",
    "    if scenario is None:\n",
    "        fig.suptitle('Probability Plots of CMIP6 and ERA5-Land ' + var_label + ' (' + starty + '-' + endy + ')',\n",
    "                 fontweight='bold', fontsize=20)\n",
    "    else:\n",
    "        fig.suptitle('Probability Plots of CMIP6 (' + scenario + ') and ERA5-Land ' + var_label +\n",
    "                     ' (' + starty + '-' + endy + ')', fontweight='bold', fontsize=20)\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    if saveas is not None:\n",
    "        plt.savefig(dir_figures+saveas)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a1f6e-949b-425a-a699-50775b666a07",
   "metadata": {},
   "source": [
    "First we'll have a look at the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92acf92c-0068-4cb9-875a-9be933d08c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_matrix(ssp2_tas_raw, era5['temp'], ssp2_tas, scenario='SSP2', show=True, saveas='NB3_CMIP6_SSP2_probability_Temp.png')\n",
    "pp_matrix(ssp5_tas_raw, era5['temp'], ssp5_tas, scenario='SSP5', show=True, saveas='NB3_CMIP6_SSP5_probability_Temp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c87d1d-14bf-454b-98f1-28e58fa49856",
   "metadata": {},
   "source": [
    "We can see that the SDM worked very well for the temperature data, with high agreement between the target and adjusted data.\n",
    "\n",
    "Let's look at the probability curves for precipitation. Since the precipitation data is bounded at 0, but most days have very small values >0 mm, we resample the data to monthly sums to get an idea of the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca1cc497-adff-47a0-8c59-c01dc7d22b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_matrix(ssp2_pr_raw, era5['prec'], ssp2_pr, precip=True, scenario='SSP2', show=True, saveas='NB3_CMIP6_SSP2_probability_Prec.png')\n",
    "pp_matrix(ssp5_pr_raw, era5['prec'], ssp5_pr, precip=True, scenario='SSP5', show=True, saveas='NB3_CMIP6_SSP5_probability_Prec.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f3206-c884-4b20-a93a-43ae884e4ad7",
   "metadata": {},
   "source": [
    "Considering the complexity and heterogeneity of precipitation data, the performance of SDM is convincing. While the fitted data of most models deviate from the target data for low and very high values, the general distribution of monthly precipitation is well met. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f387dff-b417-4fa5-b47a-54ddb187a1f7",
   "metadata": {},
   "source": [
    "## Write CMIP6 data to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2dd50-4f29-4361-a3a7-e88f8e20eb14",
   "metadata": {},
   "source": [
    "After a thorough review of the climate scenario data, we can write the final selection to files for use in the next notebook. Since the whole ensemble results in relatively large files, we store the dictionaries in binary files. These are not human readable, but compact and fast to read and write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a1183-9f41-4daa-95c9-0ada77f845eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> In the config file you can choose between two storage options: <code>pickle</code> files are fast to read and write, but take up more disk space (<code>COMPACT_FILES = False</code>). You can use them on your local machine. <code>parquet</code> files need less disk space but take longer to read and write (<code>COMPACT_FILES = True</code>). They should be your choice in the Binder.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f683d39-06c6-4c26-b8cc-02353a36610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_tas_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a02ebadf-d9e1-4a51-96c7-cbafded16385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.helpers import dict_to_pickle, dict_to_parquet\n",
    "\n",
    "tas = {'SSP2': ssp_tas_dict['SSP2_adjusted'], 'SSP5': ssp_tas_dict['SSP5_adjusted']}\n",
    "pr = {'SSP2': ssp_pr_dict['SSP2_adjusted'], 'SSP5': ssp_pr_dict['SSP5_adjusted']}\n",
    "\n",
    "if compact_files:\n",
    "    # For storage efficiency:\n",
    "    dict_to_parquet(tas, cmip_dir + 'adjusted/tas_parquet')\n",
    "    dict_to_parquet(pr, cmip_dir + 'adjusted/pr_parquet')\n",
    "else:\n",
    "    # For speed:\n",
    "    dict_to_pickle(tas, cmip_dir + 'adjusted/tas.pickle')\n",
    "    dict_to_pickle(pr, cmip_dir + 'adjusted/pr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# refresh `output_download.zip` with data retrieved within this notebook\n",
    "shutil.make_archive('output_download', 'zip', 'output')\n",
    "print('Output folder can be download now (file output_download.zip)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29efcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
