{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad5d9f7",
   "metadata": {},
   "source": [
    "# Catchment delineation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d3368",
   "metadata": {},
   "source": [
    "In this notebook we will\n",
    "\n",
    "1. ... determine the catchment area from a given pouring point based on a digital elevation model (DEM) \n",
    "2. ... determine all glaciers within that catchment area and download the glacier characteristics\n",
    "3. ... create a glacier profile based on the ice thickness and DEM of each glacier\n",
    "\n",
    "We will use Google Earth Engine (GEE) to retrieve the DEM and to perform spatial calculations. You can use different DEMs as long as they are available in the *Google Earth Engine Data Catalog* (https://developers.google.com/earth-engine/datasets/catalog). The information of the used DEM must be specified in the `config.ini` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961e915",
   "metadata": {},
   "source": [
    "Let's start by importing required packages and defining functions/constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a99cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Earth Engine packages\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# other packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Define a function to plot the digital elevation model\n",
    "def plotFigure(data, label, cmap='Blues'):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(data, extent=grid.extent, cmap=cmap)\n",
    "    plt.colorbar(label=label)\n",
    "    plt.grid()\n",
    "    \n",
    "\n",
    "# constants\n",
    "ee_img = 'Image'\n",
    "ee_ico = 'ImageCollection'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a51e8",
   "metadata": {},
   "source": [
    "First of all, the Google Earth Engine (GEE) access must be initialized. When using it for the first time on this machine, you need to authenticate first. When using <code>mybinder.org</code> you need to authenticate everytime a new session has been launched. **Copy the generated token and paste it into the input field to proceed**.\n",
    "\n",
    "Official Google Help Guide for <code>ee.Authenticate()</code>:\n",
    "\n",
    "> Prompts you to authorize access to Earth Engine via OAuth2.\n",
    ">\n",
    "> Directs you to a authentication page on the Code Editor server at code.earthengine.google.com/client-auth. You will need to pick a Cloud Project to hold your developer configuration (OAuth Client). This can be the same Cloud Project that you already use in the Code Editor, if you have not set up an OAuth client on the project already.\n",
    ">\n",
    "> The setup page also lets you choose to make the notebook access read-only. This is recommended if you are running a notebook with code that you didn't write and which may be malicious. Any operations which try to write data will fail.\n",
    ">\n",
    "> The credentials obtained by ee.Authenticate() will be written to a persistent token stored on the local machine. ee.Initialize() will automatically use the persistent credentials, if they exist. To use service account credentials\n",
    ">\n",
    "> Source: https://developers.google.com/earth-engine/apidocs/ee-authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49688cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize GEE at the beginning of session\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()         # authenticate when using GEE for the first time\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c686e",
   "metadata": {},
   "source": [
    "New read from the `config.ini` file which is used throughout the different notebooks:\n",
    "\n",
    "- input/output folders for data imports and downloads\n",
    "- filenames (DEM, GeoPackage)\n",
    "- coordinates of the defined discharge point (Lat/Long)\n",
    "- used DEM from GEE data catalog\n",
    "- show/hide GEE map in notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c84188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import ast\n",
    "\n",
    "# read local config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get file config from config.ini\n",
    "output_folder = config['FILE_SETTINGS']['DIR_OUTPUT']\n",
    "filename = output_folder + config['FILE_SETTINGS']['DEM_FILENAME']\n",
    "output_gpkg = output_folder + config['FILE_SETTINGS']['GPKG_NAME']\n",
    "\n",
    "# get used GEE DEM, coords and other settings\n",
    "dem_config = ast.literal_eval(config['CONFIG']['DEM'])\n",
    "y, x = ast.literal_eval(config['CONFIG']['COORDS'])\n",
    "show_map = config.getboolean('CONFIG','SHOW_MAP')\n",
    "\n",
    "# print config data\n",
    "print(f'Used DEM: {dem_config[3]}')\n",
    "print(f'Coordinates of discharge point: Lat {y}, Long {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e5e86",
   "metadata": {},
   "source": [
    "## Start GEE and download DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d556010",
   "metadata": {},
   "source": [
    "Now we are ready to go. Let's start with the base map if enabled in `config.ini`. The individual steps can be traced using the map since more and more layers will be added through the course of the notebook. <a id=\"map\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_map:\n",
    "    Map = geemap.Map()\n",
    "    display(Map)\n",
    "else:\n",
    "    print(\"Map view disabled in config.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb5d42",
   "metadata": {},
   "source": [
    "Load the defined DEM from GEE catalog and add is as a new layer to map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dem_config[0] == ee_img:\n",
    "    image = ee.Image(dem_config[1]).select(dem_config[2])\n",
    "elif dem_config[0] == ee_ico:\n",
    "    image = ee.ImageCollection(dem_config[1]).select(dem_config[2]).mosaic()\n",
    "\n",
    "if show_map:\n",
    "    srtm_vis = { 'bands': dem_config[2],\n",
    "                 'min': 0,\n",
    "                 'max': 6000,\n",
    "                'palette': ['000000', '478FCD', '86C58E', 'AFC35E', '8F7131','B78D4F', 'E2B8A6', 'FFFFFF']\n",
    "               }\n",
    "\n",
    "    Map.addLayer(image, srtm_vis, dem_config[3], True, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c13ec",
   "metadata": {},
   "source": [
    "Add configured discharge point to map and automatically draw box with **40km** in all directions. \n",
    "\n",
    "**<font color=\"red\">Attention!</font>** Please check whether automatically added box seems reasonable. Alternatively, a manual box can be drawn which will be considered in the next step for the catchment deliniation. **The catchment area will be cropped if the selected box is too small.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6900fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "point = ee.Geometry.Point(x,y)\n",
    "box = point.buffer(40000).bounds()\n",
    "\n",
    "if show_map:\n",
    "    Map.addLayer(point,{'color': 'blue'},'Discharge Point')\n",
    "    Map.addLayer(box,{'color': 'grey'},'Catchment Area', True, 0.7)\n",
    "    Map.centerObject(box, zoom=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4aac49",
   "metadata": {},
   "source": [
    "The discharge point (marker) and box (polygon/rectangle) can be added manually to the map above. If features have been drawn, they will overrule the configured discharge point and automatically created box.\n",
    "\n",
    "<a id=\"rp01\">**Restart Point #1**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_map:\n",
    "    for feature in Map.draw_features:\n",
    "        f_type = feature.getInfo()['geometry']['type']\n",
    "        if f_type == 'Point':\n",
    "            point = feature.geometry()\n",
    "            print(\"Manually set pouring point will be considered\")\n",
    "        elif f_type == 'Polygon':\n",
    "            box = feature.geometry()\n",
    "            print(\"Manually drawn box will be considered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33331e98",
   "metadata": {},
   "source": [
    "New we can export the DEM as `.tif` file for the selected extent to output folder. Unfortunately, there is a file size limitation for GEE downloads. In case your selected box is too big, please adjust the extent and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.ee_export_image(image, filename=filename, scale=30, region=box, file_per_band=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713b1ca",
   "metadata": {},
   "source": [
    "## Catchment deliniation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fdeb9",
   "metadata": {},
   "source": [
    "Based on the downloaded DEM file, we can use the <code>pysheds</code> module to determine the catchment area for our defined discharge point. The result will be a raster and displayed at the end of this section.\n",
    "\n",
    "The full documentation of the <code>pysheds</code> module can be found here: https://mattbartos.com/pysheds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ff78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# GIS packages\n",
    "from pysheds.grid import Grid\n",
    "import fiona\n",
    "\n",
    "# load DEM\n",
    "DEM_file = filename\n",
    "grid = Grid.from_raster(DEM_file)\n",
    "dem = grid.read_raster(DEM_file)\n",
    "print(\"DEM loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fill depressions in DEM\n",
    "print(\"Fill depressions in DEM...\")\n",
    "flooded_dem = grid.fill_depressions(dem)\n",
    "# Resolve flats in DEM\n",
    "print(\"Resolve flats in DEM...\")\n",
    "inflated_dem = grid.resolve_flats(flooded_dem)\n",
    "\n",
    "# Specify directional mapping\n",
    "#N    NE    E    SE    S    SW    W    NW\n",
    "dirmap = (64, 128, 1, 2, 4, 8, 16, 32)\n",
    "# Compute flow directions\n",
    "print(\"Compute flow directions...\")\n",
    "fdir = grid.flowdir(inflated_dem, dirmap=dirmap)\n",
    "#catch = grid.catchment(x=x, y=y, fdir=fdir, dirmap=dirmap, xytype='coordinate')\n",
    "# Compute accumulation\n",
    "print(\"Compute accumulation...\")\n",
    "acc = grid.accumulation(fdir)\n",
    "# Snap pour point to high accumulation cell\n",
    "x_snap, y_snap = grid.snap_to_mask(acc > 1000, (x, y))\n",
    "# Delineate the catchment\n",
    "print(\"Delineate the catchment...\")\n",
    "catch = grid.catchment(x=x_snap, y=y_snap, fdir=fdir, xytype='coordinate')\n",
    "# Clip the DEM to the catchment\n",
    "print(\"Clip the DEM to the catchment...\")\n",
    "grid.clip_to(catch)\n",
    "clipped_catch = grid.view(catch)\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef318dad",
   "metadata": {},
   "source": [
    "Now let's have a look at the catchment area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "demView = grid.view(dem, nodata=np.nan)\n",
    "plotFigure(demView,'Elevation in Meters',cmap='terrain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294c54a",
   "metadata": {},
   "source": [
    "We also need the catchment area as polygon to support spatial operations. Therefore convert the catchment raster to a polygon and save the result as geopackage to the output folder. Furthermore, some catchment statistics will be printed at the end of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a571ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import transform\n",
    "\n",
    "# Create shapefile and save it\n",
    "shapes = grid.polygonize()\n",
    "\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {'LABEL': 'float:16'}\n",
    "}\n",
    "\n",
    "catchment_shape = {}\n",
    "layer_name = 'catchment_orig'\n",
    "with fiona.open(output_gpkg, 'w',\n",
    "                #driver='ESRI Shapefile',#\n",
    "                driver='GPKG',\n",
    "                layer=layer_name,\n",
    "                crs=grid.crs.srs,\n",
    "                schema=schema) as c:\n",
    "    i = 0\n",
    "    for shape, value in shapes:\n",
    "        catchment_shape = shape\n",
    "        rec = {}\n",
    "        rec['geometry'] = shape\n",
    "        rec['properties'] = {'LABEL' : str(value)}\n",
    "        rec['id'] = str(i)\n",
    "        c.write(rec)\n",
    "        i += 1 \n",
    "\n",
    "print(f\"Layer '{layer_name}' added to GeoPackage '{output_gpkg}'\\n\")\n",
    "        \n",
    "catchment_bounds = [int(np.nanmin(demView)),int(np.nanmax(demView))]\n",
    "ele_cat = float(np.nanmean(demView))\n",
    "print(f\"Catchment elevation is between {catchment_bounds[0]} m and {catchment_bounds[1]} m\")\n",
    "print(f\"Mean catchment elevation is {ele_cat:.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e9447",
   "metadata": {},
   "source": [
    "Add the polygon of the catchment area to the interactive map and calculate its area with GEE. Please scroll up to see the results on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment = ee.Geometry.Polygon(catchment_shape['coordinates'])\n",
    "if show_map:\n",
    "    Map.addLayer(catchment, {}, 'Catchment')\n",
    "\n",
    "catchment_area = catchment.area().divide(1000*1000).getInfo()\n",
    "print(f\"Catchment area is {catchment_area:.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e8f53",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Attention!</font>** Please check that there is some buffer between the catchment area and the used box (&rarr; [Jump to map](#map)). If the catchment area is close to the box outline, please extent the box and repeat the DEM download and catchment delineation (&rarr; use [Restart Point #1](#rp01)).\n",
    "\n",
    "Example:\n",
    "\n",
    "1. automatically created box for pouring point (in grey) is not sufficient to capture the entire catchment &rarr; cropped at the Eastern end\n",
    "2. manually drawn box (in blue) has been added to make sure that the catchment is not cropped &rarr; space remains on all edges\n",
    "\n",
    "![Example for Cropped Catchment](images/gee_catchment_extent.png)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a3b97",
   "metadata": {},
   "source": [
    "## Determine glaciers in catchment area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0002a5f",
   "metadata": {},
   "source": [
    "The RGI 6.0 glacier outline inventory will be used to determine all glaciers that belong to the catchment area.\n",
    "\n",
    "> The *Randolph Glacier Inventory (RGI 6.0)* is a global inventory of glacier outlines. It is supplemental to the Global Land Ice Measurements from Space initiative (GLIMS). Production of the RGI was motivated by the Fifth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC AR5). Future updates will be made to the RGI and the GLIMS Glacier Database in parallel during a transition period. As all these data are incorporated into the GLIMS Glacier Database and as download tools are developed to obtain GLIMS data in the RGI data format, the RGI will evolve into a downloadable subset of GLIMS, offering complete one-time coverage, version control, and a standard set of attributes.\n",
    ">\n",
    "> Source: https://www.glims.org/RGI/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7fcd4",
   "metadata": {},
   "source": [
    "The RGI dataset is divided into 19 so called *first-order regions*. \n",
    "\n",
    "> RGI regions were developed under only three constraints: that theyshould resemble commonly recognized glacier domains,that together they should contain all of the world’s glaciers,and that their boundaries should be simple and readilyrecognizable on a map of the world. \n",
    ">\n",
    "> Source (PDF): Pfeffer, W. Tad, Anthony A. Arendt, Andrew Bliss, Tobias Bolch, J. Graham Cogley, Alex S. Gardner, Jon-Ove Hagen, et al. 2014. “The Randolph Glacier Inventory: A Globally Complete Inventory of Glaciers.” Journal of Glaciology. International Glaciological Society. https://doi.org/10.3189/2014jog13j176.\n",
    "\n",
    "![Map of the RGI regions; the red dots indicate the glacier locations and the blue circles the location of the 254 reference WGMS glaciers used by the OGGM calibration](https://docs.oggm.org/en/v1.2.0/_images/wgms_rgi_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff9294",
   "metadata": {},
   "source": [
    "In the first step, the RGI region of the catchment area must be determined to collect the right glacier outlines in a later step. Therefore, the RGI region outlines will be downloaded from the official website and spatially joined with the catchment area outline.\n",
    "\n",
    "> Source: RGI Consortium, 2017. Randolph Glacier Inventory - A Dataset of Global Glacier Outlines, Version 6. [RGI Regions, RGI Glacier Outlines]. Boulder, Colorado USA. NSIDC: National Snow and Ice Data Center. doi: https://doi.org/10.7265/4m1f-gd79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bfaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# load catcment and RGI regions as DF\n",
    "catchment = gpd.read_file(output_gpkg, layer='catchment_orig')\n",
    "df_regions = gpd.read_file('https://www.glims.org/RGI/rgi60_files/00_rgi60_regions.zip')\n",
    "display(df_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df961b00",
   "metadata": {},
   "source": [
    "For spatial operations and calculations it is crucial to use the correct projection. Otherwise, they could produce unexpected outputs. The relevant UTM zone and band for the catchment area can be automatically determined from the coordinates of the pouring point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "from pyproj import CRS\n",
    "\n",
    "utm_zone = utm.from_latlon(y, x)\n",
    "print(f\"UTM zone '{utm_zone[2]}', band '{utm_zone[3]}'\")\n",
    "\n",
    "# get CRS based on UTM\n",
    "crs = CRS.from_dict({'proj':'utm', 'zone':utm_zone[2], 'south':False})\n",
    "\n",
    "catchment_area = catchment.to_crs(crs).area[0] / 1000 / 1000\n",
    "print(f\"Catchment area (projected) is {catchment_area:.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da1109",
   "metadata": {},
   "source": [
    "Now do spatial join between catchment area and RGI regions by using the determined projection. If the catchment area contains any glaciers, the corresponding RGI region should be determined in this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b998cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = df_regions.set_crs('EPSG:4326', allow_override=True)\n",
    "catchment = catchment.to_crs('EPSG:4326')\n",
    "df_regions_catchment = gpd.sjoin(df_regions, catchment, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "if len(df_regions_catchment.index) == 0:\n",
    "    print('No area found for catchment')\n",
    "    rgi_region = None\n",
    "elif len(df_regions_catchment.index) == 1:\n",
    "    rgi_region = df_regions_catchment.iloc[0]['RGI_CODE']\n",
    "    print(f\"Catchment belongs to RGI region {rgi_region} ({df_regions_catchment.iloc[0]['FULL_NAME']})\")\n",
    "else:\n",
    "    print(\"Catchment belongs to more than one region. This use case is not yet supported.\")\n",
    "    display(df_regions_catchment)\n",
    "    rgi_region = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2099871",
   "metadata": {},
   "source": [
    "In the next step, the glacier inventory outlines for the determined RGI region will be downloaded. A spatial join is performed to determine all glacier outlines that intersect with the catchment area.\n",
    "\n",
    "**Note**: Depending on the region and bandwidth, this might take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2aa66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "if rgi_region != None:\n",
    "    url = \"https://www.glims.org/RGI/rgi60_files/\"  # Replace with the URL of your web server\n",
    "    html_page = urllib.request.urlopen(url)\n",
    "    html_content = html_page.read().decode(\"utf-8\")\n",
    "    print('Reading Randolph Glacier Inventory 6.0 in GLIMS database...')\n",
    "\n",
    "    # Use regular expressions to find links to files\n",
    "    pattern = re.compile(r'href=\"([^\"]+\\.zip)\"')\n",
    "    file_links = pattern.findall(html_content)\n",
    "\n",
    "\n",
    "    for file in file_links:\n",
    "        splits = file.split(\"_\")\n",
    "        if splits[0] != str(rgi_region):\n",
    "            continue\n",
    "\n",
    "        # starting scanning regions\n",
    "        regionname = splits[0] + \" (\" + splits[2].split(\".\")[0] + \")\"\n",
    "        print(f'Locating glacier outlines in RGI Region {regionname}...')\n",
    "\n",
    "        # read zip into dataframe\n",
    "        print('Loading shapefiles...')\n",
    "        rgi = gpd.read_file(url+file)\n",
    "        if rgi.crs != catchment.crs:\n",
    "            print(\"CRS adjusted\")\n",
    "            catchment = catchment.to_crs(rgi.crs)\n",
    "\n",
    "        # check whether catchment intersects with glaciers of region\n",
    "        print('Do spatial join...')\n",
    "        rgi_catchment = gpd.sjoin(rgi,catchment,how='inner',predicate='intersects')\n",
    "        if len(rgi_catchment.index) > 0:\n",
    "            print(f'{len(rgi_catchment.index)} outlines loaded from RGI Region {regionname}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cb30f",
   "metadata": {},
   "source": [
    "Some glaciers do not belong to catchment but are intersecting the derived catchment area. Therefore, the percentage of the glacier will be calculated. The percentage value for each glacier will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa61f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersects selects too many. calculate percentage of glacier area that is within catchment\n",
    "rgi_catchment['rgi_area'] = rgi_catchment.to_crs(crs).area    \n",
    "    \n",
    "gdf_joined = gpd.overlay(catchment, rgi_catchment, how='union')\n",
    "gdf_joined['area_joined'] = gdf_joined.to_crs(crs).area\n",
    "gdf_joined['share_of_area'] = (gdf_joined['area_joined'] / gdf_joined['rgi_area'] * 100)\n",
    "\n",
    "results = (gdf_joined\n",
    "           .groupby(['RGIId', 'LABEL_1'])\n",
    "           .agg({'share_of_area': 'sum'}))\n",
    "\n",
    "display(results.sort_values(['share_of_area'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3f9d8",
   "metadata": {},
   "source": [
    "The next step, we need to filter on the glaciers and decide whether they belong to the catchment or not. This is done based on the percentage area of the shared area. After the filtering, the catchment area will be adjusted as follows:\n",
    "\n",
    "- include glaciers where &#8805;50% of the area is part of the catchment &rarr; extend catchment area by glacier outlines (if needed)\n",
    "- exclude glaciers where <50% of the area is part of the catchment &rarr; reduce catchment area by glaicer outlines (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_catchment_merge = pd.merge(rgi_catchment, results, on=\"RGIId\")\n",
    "rgi_in_catchment = rgi_catchment_merge.loc[rgi_catchment_merge['share_of_area'] >= 50]\n",
    "rgi_out_catchment = rgi_catchment_merge.loc[rgi_catchment_merge['share_of_area'] < 50]\n",
    "catchment_new = gpd.overlay(catchment, rgi_out_catchment, how='difference')\n",
    "catchment_new = gpd.overlay(catchment_new, rgi_in_catchment, how='union')\n",
    "catchment_new = catchment_new.dissolve()[['LABEL_1', 'geometry']]\n",
    "\n",
    "print(f'Total number of determined glacier outlines: {len(rgi_catchment_merge)}')\n",
    "print(f'Number of included glacier outlines (overlap >= 50%): {len(rgi_in_catchment)}')\n",
    "print(f'Number of excluded glacier outlines (overlap < 50%): {len(rgi_out_catchment)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c658c-ec0c-4340-bc96-fc9093366f12",
   "metadata": {},
   "source": [
    "Now write RGI-IDs of glaciers that belong to the catchment to CSV file `Glaciers_in_catchment.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e5acf-f819-4eb5-a2e9-bdc7f0cd6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(output_folder + 'RGI').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "glacier_ids = pd.DataFrame(rgi_in_catchment)\n",
    "glacier_ids['RGIId'] = glacier_ids['RGIId'].map(lambda x: str(x).lstrip('RGI60-'))\n",
    "glacier_ids.to_csv(output_folder + 'RGI/' + 'Glaciers_in_catchment.csv', columns=['RGIId', 'GLIMSId'], index=False)\n",
    "display(glacier_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa39e8",
   "metadata": {},
   "source": [
    "Now let's do spatial calculations and determine the final size of catchment area and glacierized area within catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_new['area'] = catchment_new.to_crs(crs)['geometry'].area\n",
    "area_glac = rgi_in_catchment.to_crs(crs)['geometry'].area\n",
    "\n",
    "area_glac = area_glac.sum()/1000000\n",
    "area_cat = catchment_new.iloc[0]['area']/1000000\n",
    "cat_cent = catchment_new.to_crs(crs).centroid\n",
    "lat = cat_cent.to_crs('EPSG:4326').y[0]\n",
    "\n",
    "print(f\"New catchment area is {area_cat:.2f} km²\")\n",
    "print(f\"Glacierized catchment area is {area_glac:.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c36137",
   "metadata": {},
   "source": [
    "Export geo data to the existing geopackage:\n",
    "<ul>\n",
    "    <li>RGI glaciers within catchment</li>\n",
    "    <li>RGI glaciers outside catchment</li>\n",
    "    <li>Adjusted catchment area based in RGI glacier outlines</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c28a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_in_catchment.to_file(output_gpkg, layer='rgi_in', driver='GPKG')\n",
    "print(f\"Layer 'rgi_in' added to GeoPackage '{output_gpkg}'\")\n",
    "\n",
    "rgi_out_catchment.to_file(output_gpkg, layer='rgi_out', driver='GPKG')\n",
    "print(f\"Layer 'rgi_out' added to GeoPackage '{output_gpkg}'\")\n",
    "\n",
    "catchment_new.to_file(output_gpkg, layer='catchment_new', driver='GPKG')\n",
    "print(f\"Layer 'catchment_new' added to GeoPackage '{output_gpkg}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f710c3",
   "metadata": {},
   "source": [
    "The determined glacier outlines (within catchment) as well as the new catchment area can now be added to the interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220513bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_new = geemap.geopandas_to_ee(catchment_new)\n",
    "rgi = geemap.geopandas_to_ee(rgi_in_catchment)\n",
    "\n",
    "if show_map:\n",
    "    Map.addLayer(c_new, {'color': 'orange'}, \"Catchment New\")\n",
    "    Map.addLayer(rgi, {'color': 'white'}, \"RGI60\")\n",
    "    print('New layers added.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507023de",
   "metadata": {},
   "source": [
    "&rarr; [Jump to map](#map) to see results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cb7a",
   "metadata": {},
   "source": [
    "Create a simple plot to show the catchment area and the glaciers that are located within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "catchment_new.plot(color='tan',ax=ax)\n",
    "rgi_in_catchment.plot(color=\"white\",edgecolor=\"black\",ax=ax)\n",
    "plt.scatter(x, y, facecolor='blue', s=100)\n",
    "plt.title(\"Catchment Area with Pouring Point and Glaciers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21492a",
   "metadata": {},
   "source": [
    "After adding the new catchment area to GEE, we can easily calculate the mean catchment elevation in meters above sea level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39088d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_cat = image.reduceRegion(ee.Reducer.mean(),\n",
    "                          geometry=c_new).getInfo()[dem_config[2]] \n",
    "print(f\"Mean catchment elevation (adjusted) is {ele_cat:.2f} m a.s.l.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b37178",
   "metadata": {},
   "source": [
    "**Interim result**: we have determined the catchment area and we know which glaciers belong to the catchment. The next step, we need to create a glacier profile, i.e. how much ice is stored at what altitude.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065e647",
   "metadata": {},
   "source": [
    "## Retrieve raster files for ice thickness and corresponding DEM raster files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ee8f7",
   "metadata": {},
   "source": [
    "> Knowledge of the ice thickness distribution of the world’s glaciers is a fundamental prerequisite for a range of studies. Projections of future glacier change, estimates of the available freshwater resources or assessments of potential sea-level rise all need glacier ice thickness to be accurately constrained.\n",
    ">\n",
    "> Source: Farinotti, D., Huss, M., Fürst, J.J. et al. A consensus estimate for the ice thickness distribution of all glaciers on Earth. Nat. Geosci. 12, 168–173 (2019). https://doi.org/10.1038/s41561-019-0300-3\n",
    "\n",
    "The authors of the article also published a repository providing\n",
    "\n",
    "> (a) the ice thickness distribution of individual glaciers,<br/>\n",
    "> (b) global grids at various resolutions with summary-information about glacier number, area, and volume, and<br/> \n",
    "> (c) the digital elevation models of the glacier surfaces used to produce the estimates.\n",
    "> \n",
    "> Nomenclature for glaciers and regions follows the Randolph Glacier Inventory (RGI) version 6.0.\n",
    ">\n",
    "> Source: https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/315707/README.txt\n",
    "\n",
    "The ice thickness (a) and the DEM of the glacier surface (c) are input data for out glacier profile. Both information have to be  determined from raster files. The raster files will be downloaded from the server for each glacier (using RGI-ID) and stored in the output folder. The thinkness raster files will be supported by DEM raster files for easier processing.\n",
    "\n",
    "Since the original file archives are very big, the dataset is cut into smaller slices to increase performance and reuploaded to a media server. The original files are published by ETH Zürich and can be found here: https://www.research-collection.ethz.ch/handle/20.500.11850/315707\n",
    "\n",
    "In a first step, the relevant archives for DEM/thickness need to be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a296f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArchiveNames(row):\n",
    "    region = row['RGIId'].split('.')[0]\n",
    "    id = (int(row['RGIId'].split('.')[1]) - 1) // 1000 + 1\n",
    "    return f'ice_thickness_RGI60-{region}_{id}', f'dem_surface_DEM_RGI60-{region}_{id}'\n",
    "\n",
    "\n",
    "# determine relevant .zip files for derived RGI IDs \n",
    "df_rgiids = pd.DataFrame(rgi_in_catchment['RGIId'].sort_values())\n",
    "df_rgiids[['thickness', 'dem']] = df_rgiids.apply(getArchiveNames, axis=1, result_type='expand')\n",
    "zips_thickness = df_rgiids['thickness'].drop_duplicates()\n",
    "zips_dem = df_rgiids['dem'].drop_duplicates()\n",
    "\n",
    "print(f'Thickness archives:\\t{zips_thickness.tolist()}')\n",
    "print(f'DEM archives:\\t\\t{zips_dem.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc44c03",
   "metadata": {},
   "source": [
    "The archives are stored on a media server with specific references. Find the right resource references for the previously determined archives in the next step. The login data and API key must be defined in the `config.ini` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1722715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resourcespace import ResourceSpace\n",
    "\n",
    "# use guest credentials to access media server \n",
    "api_base_url = config['MEDIA_SERVER']['api_base_url']\n",
    "private_key = config['MEDIA_SERVER']['private_key']\n",
    "user = config['MEDIA_SERVER']['user']\n",
    "\n",
    "myrepository = ResourceSpace(api_base_url, user, private_key)\n",
    "\n",
    "# get resource IDs for each .zip file\n",
    "refs_thickness = pd.DataFrame(myrepository.get_collection_resources(12))[['ref', 'file_size', 'file_extension', 'field8']]\n",
    "refs_dem = pd.DataFrame(myrepository.get_collection_resources(21))[['ref', 'file_size', 'file_extension', 'field8']]\n",
    "\n",
    "# reduce list of resources two required zip files \n",
    "refs_thickness = pd.merge(zips_thickness, refs_thickness, left_on='thickness', right_on='field8')\n",
    "refs_dem = pd.merge(zips_dem, refs_dem, left_on='dem', right_on='field8')\n",
    "\n",
    "print(f'Thickness archive references:')\n",
    "display(refs_thickness)\n",
    "print(f'DEM archive references:')\n",
    "display(refs_dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6659f",
   "metadata": {},
   "source": [
    "**Ice thickness**: download relevant archives from server and extract `.tif` files to output folder.\n",
    "\n",
    "> Contains the ice thickness distribution of all glaciers of the RGI as estimated by the ensemble of considered models. Ice thicknesses are given in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import io\n",
    "\n",
    "cnt_thickness = 0\n",
    "file_names_thickness = []\n",
    "for idx, row in refs_thickness.iterrows():\n",
    "    content = myrepository.get_resource_file(row['ref'])    \n",
    "    with ZipFile(io.BytesIO(content), 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        listOfFileNames = zipObj.namelist()\n",
    "        for rgiid in df_rgiids.loc[df_rgiids['thickness'] == row['field8']]['RGIId']:\n",
    "            filename = 'RGI60-' + rgiid + '_thickness.tif'\n",
    "            if filename in listOfFileNames:\n",
    "                cnt_thickness += 1\n",
    "                zipObj.extract(filename, output_folder+'RGI')\n",
    "                file_names_thickness.append(filename)\n",
    "            else:\n",
    "                print(f'File not found: {filename}')\n",
    "                \n",
    "print(f'{cnt_thickness} files have been extracted (ice thickness)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023593d6",
   "metadata": {},
   "source": [
    "**DEM**: download relevant archives from server and extract `.tif` files to output folder.\n",
    "\n",
    "> Contains the digital elevation models (DEMs) of the glacier surfaces, as used during the calculations. The DEMs have the same extent and resolution as the corresponding ice thickness data, thus allowing for a subglacial topography to be computed (subtract the thickness from the surface DEM). Surface elevations are given in meters above sea level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cnt_dem = 0\n",
    "file_names_dem = []\n",
    "for idx,row in refs_dem.iterrows():   \n",
    "    content = myrepository.get_resource_file(row['ref'])    \n",
    "    with ZipFile(io.BytesIO(content), 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        listOfFileNames = zipObj.namelist()\n",
    "        for rgiid in df_rgiids.loc[df_rgiids['dem']==row['field8']]['RGIId']:\n",
    "            filename = f\"surface_DEM_RGI60-{rgiid}.tif\"\n",
    "            if filename in listOfFileNames:\n",
    "                cnt_dem += 1\n",
    "                zipObj.extract(filename, output_folder+'RGI')\n",
    "                file_names_dem.append(filename)\n",
    "            else:\n",
    "                print(f'File not found: {filename}')\n",
    "                \n",
    "print(f'{cnt_dem} files have been extracted (DEM)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedc469",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Attention!</font>** Check that all files have been extracted to the output folder (i.e. no error message printed) and that the number of files matches the number of glaciers within the catchment area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e317a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(rgi_in_catchment) == cnt_thickness == cnt_dem:\n",
    "    print(f\"Number of files matches the number of glaciers within catchment: {len(rgi_in_catchment)}\")\n",
    "else:\n",
    "    print(\"There is a mismatch of extracted files. Please check previous steps for error messages!\")\n",
    "    print(f'Number of included glaciers:\\t{len(rgi_in_catchment)}')\n",
    "    print(f'Ice thickness files:\\t\\t{cnt_thickness}')\n",
    "    print(f'DEM files:\\t\\t\\t{cnt_dem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb279f3a",
   "metadata": {},
   "source": [
    "## Glacier profile creation\n",
    "\n",
    "The glacier profile is used to pass the distribution of ice mass in the catchment to the glacio-hydrological model in Notebook 4. The model will calculate the annual mass balance and re-distribute the ice mass accordingly.\n",
    "\n",
    "As a first step, we need to overlay the ice thickness and DEM raster for each glacier to create tuples. By this, we now know the ice thickness of a given glacier at a particular altitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "if cnt_thickness != cnt_dem:\n",
    "    print('Number of ice thickness raster files does not match number of DEM raster files!')\n",
    "else:\n",
    "    for idx, rgiid in enumerate(df_rgiids['RGIId']):\n",
    "        if rgiid in file_names_thickness[idx] and rgiid in file_names_dem[idx]:\n",
    "            file_list = [\n",
    "                output_folder + 'RGI/' + file_names_thickness[idx],\n",
    "                output_folder + 'RGI/' + file_names_dem[idx]\n",
    "            ]\n",
    "            array_list = []\n",
    "\n",
    "            # Read arrays\n",
    "            for file in file_list:\n",
    "                src = gdal.Open(file)\n",
    "                geotransform = src.GetGeoTransform() # Could be done more elegantly outside the for loop\n",
    "                projection = src.GetProjectionRef()\n",
    "                array_list.append(src.ReadAsArray())\n",
    "                pixelSizeX = geotransform[1]\n",
    "                pixelSizeY =-geotransform[5]                \n",
    "                src = None\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            df['thickness'] = array_list[0].flatten()\n",
    "            df['altitude'] = array_list[1].flatten()\n",
    "            df_all = pd.concat([df_all, df])\n",
    "        else:\n",
    "            print(f'Raster files do not match for {rgiid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b91bc2",
   "metadata": {},
   "source": [
    "Now, remove all data points with zero ice thickness (not relevant for glaicer profile) and aggregate all data points to 10m elevation zones. In the next step, the **water equivalent** (WE) can be calculated from the average ice thickness of a data point.\n",
    "\n",
    "The result is exported to the output folder as CSV file `glacier_profile.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_all) > 0:\n",
    "    df_all = df_all.loc[df_all['thickness'] > 0]\n",
    "    df_all.sort_values(by=['altitude'],inplace=True)\n",
    "    \n",
    "    # get min/max altitude considering catchment and all glaciers\n",
    "    alt_min = 10*int(min(catchment_bounds[0],df_all['altitude'].min())/10)\n",
    "    alt_max = max(catchment_bounds[1],df_all['altitude'].max())+10\n",
    "        \n",
    "    # create bins in 10m steps\n",
    "    bins = np.arange(alt_min, df_all['altitude'].max()+10, 10)\n",
    "    \n",
    "    # aggregate per bin and do some math\n",
    "    df_agg = df_all.groupby(pd.cut(df_all['altitude'], bins))['thickness'].agg(count='size', mean='mean').reset_index()\n",
    "    df_agg['Elevation'] = df_agg['altitude'].apply(lambda x: x.left)\n",
    "    df_agg['Area'] = df_agg['count']*pixelSizeX*pixelSizeY / catchment_new.iloc[0]['area']\n",
    "    df_agg['WE'] = df_agg['mean']*0.908*1000\n",
    "    df_agg['EleZone'] = df_agg['Elevation'].apply(lambda x: 100*int(x/100))\n",
    "    \n",
    "    # delete empty elevation bands but keep at least one entry per elevation zone\n",
    "    df_agg=pd.concat([df_agg.loc[df_agg['count']>0],\n",
    "                      df_agg.loc[df_agg['count']==0].drop_duplicates(['EleZone'],keep='first')]\n",
    "                    ).sort_index()\n",
    "    \n",
    "    df_agg.drop(['altitude', 'count', 'mean'], axis=1, inplace=True)\n",
    "    df_agg = df_agg.replace(np.nan, 0)\n",
    "    df_agg.to_csv(output_folder + 'glacier_profile.csv', header=True, index=False)\n",
    "    print('Glacier profile for catchment successfully created!')\n",
    "    display(df_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1645633",
   "metadata": {},
   "source": [
    "Let's visualize the glacier profile by plotting the calculated glacier mass (in Mt) for each elevation zone. The aggregation level can be adjusted by the variable `steps` which is set to 20m by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation level for plot -> feel free to adjust\n",
    "steps = 20\n",
    "\n",
    "# get elevation range where glaciers are present\n",
    "we_range = df_agg.loc[df_agg['WE'] > 0]['Elevation']\n",
    "we_range.min() // steps * steps\n",
    "plt_zones = pd.Series(range(int(we_range.min() // steps * steps), \n",
    "                            int(we_range.max() // steps * steps + steps), \n",
    "                            steps), name='EleZone').to_frame().set_index('EleZone')\n",
    "\n",
    "# calculate glacier mass and aggregate glacier profile to defined elevation steps\n",
    "plt_data = df_agg.copy()\n",
    "plt_data['EleZone'] = plt_data['Elevation'].apply(lambda x: int(x // steps * steps))\n",
    "plt_data['Mass'] = plt_data['Area'] * catchment_new.iloc[0]['area'] * plt_data['WE'] * 1e-9 # mass in Mt\n",
    "plt_data = plt_data.drop(['Area', 'WE'], axis=1).groupby('EleZone').sum().reset_index().set_index('EleZone')\n",
    "plt_data = plt_zones.join(plt_data)\n",
    "display(plt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503d27d",
   "metadata": {},
   "source": [
    "Create plot showing the glacier mass distribution over elevation zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba511cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "plt_data.plot.barh(y='Mass', ax=ax)\n",
    "ax.set_xlabel(\"Glacier mass [Mt]\")\n",
    "ax.set_yticks(ax.get_yticks()[::int(100/steps)])\n",
    "ax.set_ylabel(\"Elevation zone [m a.s.l.]\")\n",
    "ax.get_legend().remove()\n",
    "plt.title(\"Initial Ice Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b7867",
   "metadata": {},
   "source": [
    "Calculate average glacier elevation in meters above sea level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_glac = round(df_all.altitude.mean(), 2)\n",
    "print(f'Average glacier elevation in the catchment: {ele_glac:.2f} m a.s.l.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972461e",
   "metadata": {},
   "source": [
    "# Store calculated values for other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07b15b",
   "metadata": {},
   "source": [
    "Create a `settings.yml` and store the relevant catchment information. Those information will be used in later notebooks:\n",
    "\n",
    "- **area_cat**: area of catchment in km²\n",
    "- **ele_cat**: average elevation of catchment in m.a.s.l.\n",
    "- **area_glac**: area of glacier in km²\n",
    "- **ele_glac**: average elevation of glaciers in m.a.s.l.\n",
    "- **lat**: latitude of catchment centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "settings = {'area_cat': float(area_cat),\n",
    "            'ele_cat': float(ele_cat),\n",
    "            'area_glac': float(area_glac),\n",
    "            'ele_glac': float(ele_glac),\n",
    "            'lat': float(lat)\n",
    "           }\n",
    "with open(output_folder + 'settings.yml', 'w') as f:\n",
    "    yaml.safe_dump(settings, f)\n",
    "\n",
    "print('Settings saved to file.')\n",
    "display(pd.DataFrame(settings.items(),columns=['Parameter','Value']).set_index('Parameter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1faa03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd714c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
