{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4c62e68b7d2a8b4c",
   "metadata": {},
   "source": [
    "# Catchment delineation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d3368",
   "metadata": {},
   "source": [
    "We start our workflow by downloading all the static data we need. In this notebook we will\n",
    "\n",
    "1. ...download a **Digital Elevation Model** (DEM) for hydrologic applications,\n",
    "\n",
    "2. ...**delineate the catchment** and determine the **catchment area** using your reference position (e.g. the location of your gauging station) as the \"pouring point\",\n",
    "\n",
    "3. ...identify all glaciers within the catchment and download the **glacier outlines and ice thicknesses**,\n",
    "\n",
    "4. ...create a glacier profile based on elevation zones.\n",
    "\n",
    "The DEM will be downloaded from Google Earth Engine (GEE). The default is the [MERIT DEM] (https://developers.google.com/earth-engine/datasets/catalog/MERIT_DEM_v1_0_3), but you can use any DEM available in the *Google Earth Engine Data Catalog* (https://developers.google.com/earth-engine/datasets/catalog) by specifying it in the `config.ini` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961e915",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary packages and defining functions/constants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a51e8",
   "metadata": {},
   "source": [
    "First of all, the Google Earth Engine (GEE) access must be initialized. If this is the first time you run the notebook on this machine, you need to authenticate. When using <code>mybinder.org</code> you need to authenticate every time a new session has been launched. **Follow the instructions to log in to GEE, copy the generated token, and paste it into the input field to proceed**.\n",
    "\n",
    "Official Google Help Guide for <code>ee.Authenticate()</code>:\n",
    "\n",
    "> Prompts you to authorize access to Earth Engine via OAuth2.\n",
    ">\n",
    "> Directs you to a authentication page on the Code Editor server at code.earthengine.google.com/client-auth. You will need to pick a Cloud Project to hold your developer configuration (OAuth Client). This can be the same Cloud Project that you already use in the Code Editor, if you have not set up an OAuth client on the project already.\n",
    ">\n",
    "> The setup page also lets you choose to make the notebook access read-only. This is recommended if you are running a notebook with code that you didn't write and which may be malicious. Any operations which try to write data will fail.\n",
    ">\n",
    "> The credentials obtained by ee.Authenticate() will be written to a persistent token stored on the local machine. ee.Initialize() will automatically use the persistent credentials, if they exist. To use service account credentials\n",
    ">\n",
    "> Source: https://developers.google.com/earth-engine/apidocs/ee-authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49688cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# initialize GEE at the beginning of session\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()         # authenticate when using GEE for the first time\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c686e",
   "metadata": {},
   "source": [
    "Now we will read some settings from the `config.ini` file:\n",
    "\n",
    "- **input/output** folders for data imports and downloads\n",
    "- **filenames** (DEM, GeoPackage)\n",
    "- **coordinates** of the defined \"pouring\" point (Lat/Long)\n",
    "- chosen **DEM** from GEE data catalog\n",
    "- **show/hide GEE map** in notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c84188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "# read local config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get file config from config.ini\n",
    "output_folder = config['FILE_SETTINGS']['DIR_OUTPUT']\n",
    "figures_folder = config['FILE_SETTINGS']['DIR_FIGURES']\n",
    "filename = output_folder + config['FILE_SETTINGS']['DEM_FILENAME']\n",
    "output_gpkg = output_folder + config['FILE_SETTINGS']['GPKG_NAME']\n",
    "\n",
    "# get used GEE DEM, coords and other settings\n",
    "dem_config = ast.literal_eval(config['CONFIG']['DEM'])\n",
    "y, x = ast.literal_eval(config['CONFIG']['COORDS'])\n",
    "show_map = config.getboolean('CONFIG','SHOW_MAP')\n",
    "\n",
    "# get style for matplotlib plots\n",
    "plt_style = ast.literal_eval(config['CONFIG']['PLOT_STYLE'])\n",
    "plt.style.use(plt_style)\n",
    "\n",
    "# print config data\n",
    "print(f'Used DEM: {dem_config[3]}')\n",
    "print(f'Coordinates of discharge point: Lat {y}, Lon {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e5e86",
   "metadata": {},
   "source": [
    "## Start GEE and download DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d556010",
   "metadata": {},
   "source": [
    "Once we are set up, we can start working with the data. Let's start with the **base map** if enabled in `config.ini`. The individual steps can be traced using the map since more and more layers will be added through the course of the notebook. <a id=\"map\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff7e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "\n",
    "if show_map:\n",
    "    Map = geemap.Map()\n",
    "    display(Map)\n",
    "else:\n",
    "    print(\"Map view disabled in config.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb5d42",
   "metadata": {},
   "source": [
    "Now we can add the DEM from the GEE catalog and add it as a new layer to the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce4b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dem_config[0] == 'Image':\n",
    "    image = ee.Image(dem_config[1]).select(dem_config[2])\n",
    "elif dem_config[0] == 'ImageCollection':\n",
    "    image = ee.ImageCollection(dem_config[1]).select(dem_config[2]).mosaic()\n",
    "\n",
    "if show_map:\n",
    "    srtm_vis = { 'bands': dem_config[2],\n",
    "                 'min': 0,\n",
    "                 'max': 6000,\n",
    "                'palette': ['000000', '478FCD', '86C58E', 'AFC35E', '8F7131','B78D4F', 'E2B8A6', 'FFFFFF']\n",
    "               }\n",
    "\n",
    "    Map.addLayer(image, srtm_vis, dem_config[3], True, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c13ec",
   "metadata": {},
   "source": [
    "Next, we add the configured discharge point to the map and generate a **40km** buffer box. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Please check whether the default box covers your research area. Alternatively, you can adjust the box manually. <b>The catchment area will be cropped if the selected box is too small.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6900fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "point = ee.Geometry.Point(x,y)\n",
    "box = point.buffer(40000).bounds()\n",
    "\n",
    "if show_map:\n",
    "    Map.addLayer(point,{'color': 'blue'},'Discharge Point')\n",
    "    Map.addLayer(box,{'color': 'grey'},'Catchment Area', True, 0.7)\n",
    "    Map.centerObject(box, zoom=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4aac49",
   "metadata": {},
   "source": [
    "The discharge point (marker) and box (polygon/rectangle) can also be added manually to the map above. If features have been drawn, they will overrule the configured discharge point and automatically created box.\n",
    "\n",
    "<a id=\"rp01\">**Restart Point #1**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5cd6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_map:\n",
    "    for feature in Map.draw_features:\n",
    "        f_type = feature.getInfo()['geometry']['type']\n",
    "        if f_type == 'Point':\n",
    "            point = feature.geometry()\n",
    "            print(\"Manually set pouring point will be considered\")\n",
    "        elif f_type == 'Polygon':\n",
    "            box = feature.geometry()\n",
    "            print(\"Manually drawn box will be considered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33331e98",
   "metadata": {},
   "source": [
    "Now we can export the DEM as a `.tif` file for the selected extent to the output folder. Unfortunately there is a file size limit for GEE downloads. If your selected box is too big, please adjust the extent and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e05aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.ee_export_image(image, filename=filename, scale=30, region=box, file_per_band=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713b1ca",
   "metadata": {},
   "source": [
    "## Catchment deliniation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fdeb9",
   "metadata": {},
   "source": [
    "Based on the downloaded DEM file, we can calculate a catchment area using the <code>pysheds</code> library. The result will be a raster and displayed at the end of this section.\n",
    "\n",
    "The full documentation of the <code>pysheds</code> module can be found [here](https://mattbartos.com/pysheds/).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The catchment delineation involves several steps with large array operations and can take a moment.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2ff78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# GIS packages\n",
    "from pysheds.grid import Grid\n",
    "import fiona\n",
    "\n",
    "# load DEM\n",
    "DEM_file = filename\n",
    "grid = Grid.from_raster(DEM_file)\n",
    "dem = grid.read_raster(DEM_file)\n",
    "print(\"DEM loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2014aa34",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fill depressions in DEM\n",
    "print(\"Fill depressions in DEM...\")\n",
    "flooded_dem = grid.fill_depressions(dem)\n",
    "# Resolve flats in DEM\n",
    "print(\"Resolve flats in DEM...\")\n",
    "inflated_dem = grid.resolve_flats(flooded_dem)\n",
    "\n",
    "# Specify directional mapping\n",
    "#N    NE    E    SE    S    SW    W    NW\n",
    "dirmap = (64, 128, 1, 2, 4, 8, 16, 32)\n",
    "# Compute flow directions\n",
    "print(\"Compute flow directions...\")\n",
    "fdir = grid.flowdir(inflated_dem, dirmap=dirmap)\n",
    "#catch = grid.catchment(x=x, y=y, fdir=fdir, dirmap=dirmap, xytype='coordinate')\n",
    "# Compute accumulation\n",
    "print(\"Compute accumulation...\")\n",
    "acc = grid.accumulation(fdir)\n",
    "# Snap pour point to high accumulation cell\n",
    "x_snap, y_snap = grid.snap_to_mask(acc > 1000, (x, y))\n",
    "# Delineate the catchment\n",
    "print(\"Delineate the catchment...\")\n",
    "catch = grid.catchment(x=x_snap, y=y_snap, fdir=fdir, xytype='coordinate')\n",
    "# Clip the DEM to the catchment\n",
    "print(\"Clip the DEM to the catchment...\")\n",
    "grid.clip_to(catch)\n",
    "clipped_catch = grid.view(catch)\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef318dad",
   "metadata": {},
   "source": [
    "Now let's have a look at the catchment area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906b11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to plot the digital elevation model\n",
    "def plotFigure(data, label, cmap='Blues'):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(data, extent=grid.extent, cmap=cmap)\n",
    "    plt.colorbar(label=label)\n",
    "    plt.grid()\n",
    "\n",
    "demView = grid.view(dem, nodata=np.nan)\n",
    "plotFigure(demView,'Elevation in Meters',cmap='terrain')\n",
    "plt.savefig(figures_folder+'NB1_DEM_Catchment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294c54a",
   "metadata": {},
   "source": [
    "For the following steps we need the catchment outline as a polygon. Thus, we convert the **raster to a polygon** and save both in a **geopackage** to the output folder. From these files, we can already calculate important **catchment statistics** needed for the glacio-hydrological model in Notebook 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75a571ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, shape\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "\n",
    "# Create shapefile and save it\n",
    "shapes = grid.polygonize()\n",
    "\n",
    "schema = {\n",
    "    'geometry': 'Polygon',\n",
    "    'properties': {'LABEL': 'float:16'}\n",
    "}\n",
    "\n",
    "catchment_shape = {}\n",
    "layer_name = 'catchment_orig'\n",
    "with fiona.open(output_gpkg, 'w',\n",
    "                #driver='ESRI Shapefile',#\n",
    "                driver='GPKG',\n",
    "                layer=layer_name,\n",
    "                crs=grid.crs.srs,\n",
    "                schema=schema) as c:\n",
    "    i = 0\n",
    "    for shape, value in shapes:\n",
    "        catchment_shape = shape\n",
    "        rec = {}\n",
    "        rec['geometry'] = shape\n",
    "        rec['properties'] = {'LABEL' : str(value)}\n",
    "        rec['id'] = str(i)\n",
    "        c.write(rec)\n",
    "        i += 1 \n",
    "\n",
    "print(f\"Layer '{layer_name}' added to GeoPackage '{output_gpkg}'\\n\")\n",
    "        \n",
    "catchment_bounds = [int(np.nanmin(demView)),int(np.nanmax(demView))]\n",
    "ele_cat = float(np.nanmean(demView))\n",
    "print(f\"Catchment elevation ranges from {catchment_bounds[0]} m to {catchment_bounds[1]} m.a.s.l.\")\n",
    "print(f\"Mean catchment elevation is {ele_cat:.2f} m.a.s.l.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e9447",
   "metadata": {},
   "source": [
    "We can also add the catchment polygon to the interactive map. This sends it to GEE and allows us to use a GEE function to calculate its area. Please scroll up to see the results on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfc5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment = ee.Geometry.Polygon(catchment_shape['coordinates'])\n",
    "if show_map:\n",
    "    Map.addLayer(catchment, {}, 'Catchment')\n",
    "\n",
    "catchment_area = catchment.area().divide(1000*1000).getInfo()\n",
    "print(f\"Catchment area is {catchment_area:.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e8f53",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>\n",
    " Please make sure to leave some buffer between the catchment outline and the used box (&rarr; <a href='#map'>Jump to map</a>). If the bounding box is close to the catchment, please extent the box and repeat the DEM download and catchment delineation (&rarr; use <a href='#rp01'>Restart Point #1</a>).</div>\n",
    "\n",
    "Example:\n",
    "\n",
    "1. The automatically created box for the pouring point (in gray) is not sufficient to cover the entire catchment area; cropped at the eastern edge.\n",
    "2. Manually drawn box (in blue) has been added to ensure that the catchment is not cropped &rarr; buffer remains on all edges\n",
    "\n",
    "![Example for Cropped Catchment](images/gee_catchment_extent.png)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a3b97",
   "metadata": {},
   "source": [
    "## Determine glaciers in catchment area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0002a5f",
   "metadata": {},
   "source": [
    "To acquire outlines of all glaciers in the catchment we will rely on latest Randolph Glacier Inventory (RGI 6.0).\n",
    "\n",
    "> The *Randolph Glacier Inventory (RGI 6.0)* is a global inventory of glacier outlines. It is supplemental to the Global Land Ice Measurements from Space initiative (GLIMS). Production of the RGI was motivated by the Fifth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC AR5). Future updates will be made to the RGI and the GLIMS Glacier Database in parallel during a transition period. As all these data are incorporated into the GLIMS Glacier Database and as download tools are developed to obtain GLIMS data in the RGI data format, the RGI will evolve into a downloadable subset of GLIMS, offering complete one-time coverage, version control, and a standard set of attributes.\n",
    ">\n",
    "> Source: https://www.glims.org/RGI/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7fcd4",
   "metadata": {},
   "source": [
    "The RGI dataset is divided into 19 so called *first-order regions*. \n",
    "\n",
    "> RGI regions were developed under only three constraints: that they should resemble commonly recognized glacier domains, that together they should contain all of the world’s glaciers, and that their boundaries should be simple and readily recognizable on a map of the world. \n",
    ">\n",
    "> Source: [Pfeffer et.al. 2014](https://doi.org/10.3189/2014jog13j176)\n",
    "\n",
    "![Map of the RGI regions; the red dots indicate the glacier locations and the blue circles the location of the 254 reference WGMS glaciers used by the OGGM calibration](https://docs.oggm.org/en/v1.2.0/_images/wgms_rgi_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff9294",
   "metadata": {},
   "source": [
    "In the first step, the RGI region of the catchment area must be determined to access the correct repository. Therefore, the RGI region outlines will be downloaded from the official website and joined with the catchment outline.\n",
    "\n",
    "> Source: [RGI Consortium (2017)](https://doi.org/10.7265/4m1f-gd79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf1bfaf9",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# load catcment and RGI regions as DF\n",
    "catchment = gpd.read_file(output_gpkg, layer='catchment_orig')\n",
    "df_regions = gpd.read_file('https://www.glims.org/RGI/rgi60_files/00_rgi60_regions.zip', layer='00_rgi60_O1Regions')\n",
    "display(df_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df961b00",
   "metadata": {},
   "source": [
    "For spatial calculations it is crucial to use the correct projection. To avoid inaccuracies due to unit conversions we will project the data to UTM whenever we calculate spatial statistics. The relevant UTM zone and band for the catchment area are determined from the coordinates of the pouring point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e2f4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "from pyproj import CRS\n",
    "\n",
    "utm_zone = utm.from_latlon(y, x)\n",
    "print(f\"UTM zone '{utm_zone[2]}', band '{utm_zone[3]}'\")\n",
    "\n",
    "# get CRS based on UTM\n",
    "crs = CRS.from_dict({'proj':'utm', 'zone':utm_zone[2], 'south':False})\n",
    "\n",
    "catchment_area = catchment.to_crs(crs).area[0] / 1000 / 1000\n",
    "print(f\"Catchment area (projected) is {catchment_area:.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da1109",
   "metadata": {},
   "source": [
    "Now we can perform a spatial join between the catchment outline and the RGI regions. If the catchment contains any glaciers, the corresponding RGI region is determined in this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b998cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = df_regions.set_crs('EPSG:4326', allow_override=True)\n",
    "catchment = catchment.to_crs('EPSG:4326')\n",
    "df_regions_catchment = gpd.sjoin(df_regions, catchment, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "if len(df_regions_catchment.index) == 0:\n",
    "    print('No area found for catchment')\n",
    "    rgi_region = None\n",
    "elif len(df_regions_catchment.index) == 1:\n",
    "    rgi_region = df_regions_catchment.iloc[0]['RGI_CODE']\n",
    "    print(f\"Catchment belongs to RGI region {rgi_region} ({df_regions_catchment.iloc[0]['FULL_NAME']})\")\n",
    "else:\n",
    "    print(\"Catchment belongs to more than one region. This use case is not yet supported.\")\n",
    "    display(df_regions_catchment)\n",
    "    rgi_region = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2099871",
   "metadata": {},
   "source": [
    "In the next step, the glacier inventory outlines for the determined RGI region will be downloaded. A spatial join is performed to determine all glacier outlines that intersect with the catchment area.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    " Depending on the region and bandwidth, this might take 1 min or longer.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2aa66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import urllib.request\n",
    "import fsspec\n",
    "import re\n",
    "\n",
    "if rgi_region != None:\n",
    "    url = \"https://www.glims.org/RGI/rgi60_files/\"  # Replace with the URL of your web server\n",
    "    html_page = urllib.request.urlopen(url)\n",
    "    html_content = html_page.read().decode(\"utf-8\")\n",
    "    print('Reading Randolph Glacier Inventory 6.0 in GLIMS database...')\n",
    "\n",
    "    # Use regular expressions to find links to files\n",
    "    pattern = re.compile(r'href=\"([^\"]+\\.zip)\"')\n",
    "    file_links = pattern.findall(html_content)\n",
    "\n",
    "\n",
    "    for file in file_links:\n",
    "        splits = file.split(\"_\")\n",
    "        if splits[0] != str(rgi_region):\n",
    "            continue\n",
    "\n",
    "        # starting scanning regions\n",
    "        regionname = splits[0] + \" (\" + splits[2].split(\".\")[0] + \")\"\n",
    "        print(f'Locating glacier outlines in RGI Region {regionname}...')\n",
    "\n",
    "        # read zip into dataframe\n",
    "        print('Loading shapefiles...')\n",
    "        path = f\"simplecache::{url+file}\"\n",
    "        print('Path:', path)\n",
    "        with fsspec.open(path) as file:\n",
    "            rgi = gpd.read_file(file)\n",
    "\n",
    "        if rgi.crs != catchment.crs:\n",
    "            print(\"CRS adjusted\")\n",
    "            catchment = catchment.to_crs(rgi.crs)\n",
    "\n",
    "        # check whether catchment intersects with glaciers of region\n",
    "        print('Perform spatial join...')\n",
    "        rgi_catchment = gpd.sjoin(rgi,catchment,how='inner',predicate='intersects')\n",
    "        if len(rgi_catchment.index) > 0:\n",
    "            print(f'{len(rgi_catchment.index)} outlines loaded from RGI Region {regionname}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cb30f",
   "metadata": {},
   "source": [
    "Some glaciers are not actually in the catchment, but intersect its outline. We will first determine their fractional overlap with the target catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa61f85",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# intersects selects too many. calculate percentage of glacier area that is within catchment\n",
    "rgi_catchment['rgi_area'] = rgi_catchment.to_crs(crs).area\n",
    "    \n",
    "gdf_joined = gpd.overlay(catchment, rgi_catchment, how='union')\n",
    "gdf_joined['area_joined'] = gdf_joined.to_crs(crs).area\n",
    "gdf_joined['share_of_area'] = (gdf_joined['area_joined'] / gdf_joined['rgi_area'] * 100)\n",
    "\n",
    "results = (gdf_joined\n",
    "           .groupby(['RGIId', 'LABEL_1'])\n",
    "           .agg({'share_of_area': 'sum'}))\n",
    "\n",
    "display(results.sort_values(['share_of_area'],ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3f9d8",
   "metadata": {},
   "source": [
    "Now we can **filter** based on the percentage of shared area. After that the catchment area will be adjusted as follows:\n",
    "\n",
    "- **&#8805;50%** of the area are in the catchment &rarr; **include** and extend catchment area by full glacier outlines (if needed)\n",
    "- **<50%** of the area are in the catchment &rarr; **exclude** and reduce catchment area by glacier outlines (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f0c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_catchment_merge = pd.merge(rgi_catchment, results, on=\"RGIId\")\n",
    "rgi_in_catchment = rgi_catchment_merge.loc[rgi_catchment_merge['share_of_area'] >= 50]\n",
    "rgi_out_catchment = rgi_catchment_merge.loc[rgi_catchment_merge['share_of_area'] < 50]\n",
    "catchment_new = gpd.overlay(catchment, rgi_out_catchment, how='difference')\n",
    "catchment_new = gpd.overlay(catchment_new, rgi_in_catchment, how='union')\n",
    "catchment_new = catchment_new.dissolve()[['LABEL_1', 'geometry']]\n",
    "\n",
    "print(f'Total number of determined glacier outlines: {len(rgi_catchment_merge)}')\n",
    "print(f'Number of included glacier outlines (overlap >= 50%): {len(rgi_in_catchment)}')\n",
    "print(f'Number of excluded glacier outlines (overlap < 50%): {len(rgi_out_catchment)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c658c-ec0c-4340-bc96-fc9093366f12",
   "metadata": {},
   "source": [
    "The RGI-IDs of the remaining glaciers are stored in `Glaciers_in_catchment.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb4e5acf-f819-4eb5-a2e9-bdc7f0cd6c69",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(output_folder + 'RGI').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "glacier_ids = pd.DataFrame(rgi_in_catchment)\n",
    "glacier_ids['RGIId'] = glacier_ids['RGIId'].map(lambda x: str(x).lstrip('RGI60-'))\n",
    "glacier_ids.to_csv(output_folder + 'RGI/' + 'Glaciers_in_catchment.csv', columns=['RGIId', 'GLIMSId'], index=False)\n",
    "display(glacier_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa39e8",
   "metadata": {},
   "source": [
    "With an updated glacier outline we can now determine the **final area of the catchment** and the **part covered by glaciers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d10ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_new['area'] = catchment_new.to_crs(crs)['geometry'].area\n",
    "area_glac = rgi_in_catchment.to_crs(crs)['geometry'].area\n",
    "\n",
    "area_glac = area_glac.sum()/1000000\n",
    "area_cat = catchment_new.iloc[0]['area']/1000000\n",
    "cat_cent = catchment_new.to_crs(crs).centroid\n",
    "lat = cat_cent.to_crs('EPSG:4326').y[0]\n",
    "\n",
    "print(f\"New catchment area is {area_cat:.2f} km²\")\n",
    "print(f\"Glacierized catchment area is {area_glac:.2f} km²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c36137",
   "metadata": {},
   "source": [
    "The files just created are added to the existing geopackage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70c28a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_in_catchment.to_file(output_gpkg, layer='rgi_in', driver='GPKG')\n",
    "print(f\"Layer 'rgi_in' added to GeoPackage '{output_gpkg}'\")\n",
    "\n",
    "rgi_out_catchment.to_file(output_gpkg, layer='rgi_out', driver='GPKG')\n",
    "print(f\"Layer 'rgi_out' added to GeoPackage '{output_gpkg}'\")\n",
    "\n",
    "catchment_new.to_file(output_gpkg, layer='catchment_new', driver='GPKG')\n",
    "print(f\"Layer 'catchment_new' added to GeoPackage '{output_gpkg}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f710c3",
   "metadata": {},
   "source": [
    "...and can also be added to the interactive map..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "220513bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_new = geemap.geopandas_to_ee(catchment_new)\n",
    "rgi = geemap.geopandas_to_ee(rgi_in_catchment)\n",
    "\n",
    "if show_map:\n",
    "    Map.addLayer(c_new, {'color': 'orange'}, \"Catchment New\")\n",
    "    Map.addLayer(rgi, {'color': 'white'}, \"RGI60\")\n",
    "    print('New layers added.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507023de",
   "metadata": {},
   "source": [
    "&rarr; [Jump to map](#map) to see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cb7a",
   "metadata": {},
   "source": [
    "...or combined in a simple plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32af6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "catchment_new.plot(color='tan',ax=ax)\n",
    "rgi_in_catchment.plot(color=\"white\",edgecolor=\"black\",ax=ax)\n",
    "plt.scatter(x, y, facecolor='blue', s=100)\n",
    "plt.title(\"Catchment Area with Pouring Point and Glaciers\")\n",
    "plt.savefig(figures_folder+'NB1_Glaciers_Catchment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21492a",
   "metadata": {},
   "source": [
    "After adding the new catchment area to GEE, we can easily calculate the mean catchment elevation in meters above sea level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39088d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_cat = image.reduceRegion(ee.Reducer.mean(),\n",
    "                          geometry=c_new).getInfo()[dem_config[2]] \n",
    "print(f\"Mean catchment elevation (adjusted) is {ele_cat:.2f} m a.s.l.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b37178",
   "metadata": {},
   "source": [
    "### Interim Summary:\n",
    "\n",
    "So far we have...\n",
    "\n",
    "- ...delineated the catchment and determined its area,\n",
    "\n",
    "- ...calculated the average elevation of the catchment,\n",
    "\n",
    "- ...identified the glaciers in the catchment and calculated their combined area.\n",
    "\n",
    "In the next step, we will create a glacier profile to determine how the ice is distributed over the elevation range.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065e647",
   "metadata": {},
   "source": [
    "## Retrieve ice thickness rasters and corresponding DEM files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da1389-3833-48d8-8be2-fdd380561680",
   "metadata": {},
   "source": [
    "Determining ice thickness from remotely sensed data is a challenging task. Fortunately, [Farinotti et.al. (2019)](https://doi.org/10.1038/s41561-019-0300-3) calculated an ensemble estimate of different methods for all glaciers in RGI6 and made the data available [to the public](https://www.research-collection.ethz.ch/handle/20.500.11850/315707)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ee8f7",
   "metadata": {},
   "source": [
    "The published repository contains...\n",
    "\n",
    "> (a) the **ice thickness distribution** of individual glaciers,<br/>\n",
    "> (b) global grids at various resolutions with **summary-information about glacier number, area, and volume**, and<br/> \n",
    "> (c) the **digital elevation models** of the glacier surfaces used to produce the estimates.\n",
    "> \n",
    "> Nomenclature for glaciers and regions follows the Randolph Glacier Inventory (RGI) version 6.0.\n",
    ">\n",
    "> Source: Farinotti et.al. 2019 - [README](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/315707/README.txt)\n",
    "\n",
    "The **ice thickness rasters (a)** and **aligned DEMs (c)** are the perfect input data for our glacier profile. The files are selected and downloaded by their **RGI IDs** and stored in the output folder.\n",
    "\n",
    "Since the original files hosted by ETH Zurich are stored in large archives, we cut the dataset into smaller slices and reupload them according to the respective [license](https://creativecommons.org/licenses/by-nc-sa/4.0/) to make them searchable, improve performance, and limit traffic.\n",
    "\n",
    "First, we identify the relevant archives for our set of glacier IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a296f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArchiveNames(row):\n",
    "    region = row['RGIId'].split('.')[0]\n",
    "    id = (int(row['RGIId'].split('.')[1]) - 1) // 1000 + 1\n",
    "    return f'ice_thickness_{region}_{id}', f'dem_surface_DEM_{region}_{id}'\n",
    "\n",
    "\n",
    "# determine relevant .zip files for derived RGI IDs \n",
    "df_rgiids = pd.DataFrame(rgi_in_catchment['RGIId'].sort_values())\n",
    "df_rgiids[['thickness', 'dem']] = df_rgiids.apply(getArchiveNames, axis=1, result_type='expand')\n",
    "zips_thickness = df_rgiids['thickness'].drop_duplicates()\n",
    "zips_dem = df_rgiids['dem'].drop_duplicates()\n",
    "\n",
    "print(f'Thickness archives:\\t{zips_thickness.tolist()}')\n",
    "print(f'DEM archives:\\t\\t{zips_dem.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc44c03",
   "metadata": {},
   "source": [
    "The archives are stored on a file server at the Humboldt University of Berlin, which provides limited read access to this notebook. The corresponding credentials and API key are defined in the `config.ini` file. The next step is to identify the corresponding resource references for the previously identified archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1722715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resourcespace import ResourceSpace\n",
    "\n",
    "# use guest credentials to access media server \n",
    "api_base_url = config['MEDIA_SERVER']['api_base_url']\n",
    "private_key = config['MEDIA_SERVER']['private_key']\n",
    "user = config['MEDIA_SERVER']['user']\n",
    "\n",
    "myrepository = ResourceSpace(api_base_url, user, private_key)\n",
    "\n",
    "# get resource IDs for each .zip file\n",
    "refs_thickness = pd.DataFrame(myrepository.get_collection_resources(12))[['ref', 'file_size', 'file_extension', 'field8']]\n",
    "refs_dem = pd.DataFrame(myrepository.get_collection_resources(21))[['ref', 'file_size', 'file_extension', 'field8']]\n",
    "\n",
    "# reduce list of resources two required zip files \n",
    "refs_thickness = pd.merge(zips_thickness, refs_thickness, left_on='thickness', right_on='field8')\n",
    "refs_dem = pd.merge(zips_dem, refs_dem, left_on='dem', right_on='field8')\n",
    "\n",
    "print(f'Thickness archive references:\\n')\n",
    "display(refs_thickness)\n",
    "print(f'DEM archive references:\\n')\n",
    "display(refs_dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6659f",
   "metadata": {},
   "source": [
    "Again, depending on the number of files and bandwidth, this may take a moment. Let's start with the **ice thickness**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f58b3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import io\n",
    "\n",
    "cnt_thickness = 0\n",
    "file_names_thickness = []\n",
    "for idx, row in refs_thickness.iterrows():\n",
    "    content = myrepository.get_resource_file(row['ref'], row['file_extension'])    \n",
    "    with ZipFile(io.BytesIO(content), 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        listOfFileNames = zipObj.namelist()\n",
    "        for rgiid in df_rgiids.loc[df_rgiids['thickness'] == row['field8']]['RGIId']:\n",
    "            filename = rgiid + '_thickness.tif'\n",
    "            if filename in listOfFileNames:\n",
    "                cnt_thickness += 1\n",
    "                zipObj.extract(filename, output_folder+'RGI')\n",
    "                file_names_thickness.append(filename)\n",
    "            else:\n",
    "                print(f'File not found: {filename}')\n",
    "                \n",
    "print(f'{cnt_thickness} files have been extracted (ice thickness)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023593d6",
   "metadata": {},
   "source": [
    "...and continue with the matching **DEMs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "324fb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cnt_dem = 0\n",
    "file_names_dem = []\n",
    "for idx,row in refs_dem.iterrows():   \n",
    "    content = myrepository.get_resource_file(row['ref'])    \n",
    "    with ZipFile(io.BytesIO(content), 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        listOfFileNames = zipObj.namelist()\n",
    "        for rgiid in df_rgiids.loc[df_rgiids['dem']==row['field8']]['RGIId']:\n",
    "            filename = f\"surface_DEM_{rgiid}.tif\"\n",
    "            if filename in listOfFileNames:\n",
    "                cnt_dem += 1\n",
    "                zipObj.extract(filename, output_folder+'RGI')\n",
    "                file_names_dem.append(filename)\n",
    "            else:\n",
    "                print(f'File not found: {filename}')\n",
    "                \n",
    "print(f'{cnt_dem} files have been extracted (DEM)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedc469",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>\n",
    " Please check whether all files have been extracted to the output folder without error messages and <b>the number of files matches the number of glaciers</b>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68e317a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(rgi_in_catchment) == cnt_thickness == cnt_dem:\n",
    "    print(f\"Number of files matches the number of glaciers within catchment: {len(rgi_in_catchment)}\")\n",
    "else:\n",
    "    print(\"There is a mismatch of extracted files. Please check previous steps for error messages!\")\n",
    "    print(f'Number of included glaciers:\\t{len(rgi_in_catchment)}')\n",
    "    print(f'Ice thickness files:\\t\\t{cnt_thickness}')\n",
    "    print(f'DEM files:\\t\\t\\t{cnt_dem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb279f3a",
   "metadata": {},
   "source": [
    "## Glacier profile creation\n",
    "\n",
    "The **glacier profile** is used to pass the distribution of ice mass in the catchment to the glacio-hydrological model in **Notebook 4**, following the approach of [Seibert et.al.2018](https://doi.org/10.5194/hess-22-2211-2018). The model then calculates the annual mass balance and redistributes the ice mass accordingly.\n",
    "\n",
    "To derive the profile from spatially distributed data, we first stack the ice thickness and corresponding DEM rasters for each glacier and create tuples of ice thickness and elevation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1d8286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "if cnt_thickness != cnt_dem:\n",
    "    print('Number of ice thickness raster files does not match number of DEM raster files!')\n",
    "else:\n",
    "    for idx, rgiid in enumerate(df_rgiids['RGIId']):\n",
    "        if rgiid in file_names_thickness[idx] and rgiid in file_names_dem[idx]:\n",
    "            file_list = [\n",
    "                output_folder + 'RGI/' + file_names_thickness[idx],\n",
    "                output_folder + 'RGI/' + file_names_dem[idx]\n",
    "            ]\n",
    "            array_list = []\n",
    "\n",
    "            # Read arrays\n",
    "            for file in file_list:\n",
    "                src = gdal.Open(file)\n",
    "                geotransform = src.GetGeoTransform() # Could be done more elegantly outside the for loop\n",
    "                projection = src.GetProjectionRef()\n",
    "                array_list.append(src.ReadAsArray())\n",
    "                pixelSizeX = geotransform[1]\n",
    "                pixelSizeY =-geotransform[5]                \n",
    "                src = None\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            df['thickness'] = array_list[0].flatten()\n",
    "            df['altitude'] = array_list[1].flatten()\n",
    "            df_all = pd.concat([df_all, df])\n",
    "\n",
    "        else:\n",
    "            print(f'Raster files do not match for {rgiid}')\n",
    "\n",
    "print(\"Ice thickness and elevations rasters stacked\")\n",
    "print(\"Value pairs created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b91bc2",
   "metadata": {},
   "source": [
    "Now we can remove all data points with zero ice thickness and aggregate all data points into 10m **elevation zones**. The next step is to calculate the **water equivalent** (WE) from the average ice thickness in each elevation zone.\n",
    "\n",
    "The result is exported to the output folder as `glacier_profile.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b46872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_all) > 0:\n",
    "    df_all = df_all.loc[df_all['thickness'] > 0]\n",
    "    df_all.sort_values(by=['altitude'],inplace=True)\n",
    "    \n",
    "    # get min/max altitude considering catchment and all glaciers\n",
    "    alt_min = 10*int(min(catchment_bounds[0],df_all['altitude'].min())/10)\n",
    "    alt_max = max(catchment_bounds[1],df_all['altitude'].max())+10\n",
    "        \n",
    "    # create bins in 10m steps\n",
    "    bins = np.arange(alt_min, df_all['altitude'].max()+10, 10)\n",
    "    \n",
    "    # aggregate per bin and do some math\n",
    "    df_agg = df_all.groupby(pd.cut(df_all['altitude'], bins))['thickness'].agg(count='size', mean='mean').reset_index()\n",
    "    df_agg['Elevation'] = df_agg['altitude'].apply(lambda x: x.left).astype(int)\n",
    "    df_agg['Area'] = df_agg['count']*pixelSizeX*pixelSizeY / catchment_new.iloc[0]['area']\n",
    "    df_agg['WE'] = df_agg['mean']*0.908*1000\n",
    "    df_agg['EleZone'] = df_agg['Elevation'].apply(lambda x: 100*int(x/100))\n",
    "    \n",
    "    # delete empty elevation bands but keep at least one entry per elevation zone\n",
    "    df_agg=pd.concat([df_agg.loc[df_agg['count']>0],\n",
    "                      df_agg.loc[df_agg['count']==0].drop_duplicates(['EleZone'],keep='first')]\n",
    "                    ).sort_index()\n",
    "    \n",
    "    df_agg.drop(['altitude', 'count', 'mean'], axis=1, inplace=True)\n",
    "    df_agg = df_agg.replace(np.nan, 0)\n",
    "    df_agg.to_csv(output_folder + 'glacier_profile.csv', header=True, index=False)\n",
    "    print('Glacier profile for catchment created!\\n')\n",
    "    display(df_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1645633",
   "metadata": {},
   "source": [
    "Let's visualize the glacier profile. First we aggregate the ice mass in larger elevation zones for better visibility. The level of aggregation can be adjusted using the variable `steps` (default is 20m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3ba6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation level for plot -> feel free to adjust\n",
    "steps = 20\n",
    "\n",
    "# get elevation range where glaciers are present\n",
    "we_range = df_agg.loc[df_agg['WE'] > 0]['Elevation']\n",
    "we_range.min() // steps * steps\n",
    "plt_zones = pd.Series(range(int(we_range.min() // steps * steps), \n",
    "                            int(we_range.max() // steps * steps + steps), \n",
    "                            steps), name='EleZone').to_frame().set_index('EleZone')\n",
    "\n",
    "# calculate glacier mass and aggregate glacier profile to defined elevation steps\n",
    "plt_data = df_agg.copy()\n",
    "plt_data['EleZone'] = plt_data['Elevation'].apply(lambda x: int(x // steps * steps))\n",
    "plt_data['Mass'] = plt_data['Area'] * catchment_new.iloc[0]['area'] * plt_data['WE'] * 1e-9 # mass in Mt\n",
    "plt_data = plt_data.drop(['Area', 'WE'], axis=1).groupby('EleZone').sum().reset_index().set_index('EleZone')\n",
    "plt_data = plt_zones.join(plt_data)\n",
    "display(plt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503d27d",
   "metadata": {},
   "source": [
    "Now, we can plot the estimated **glacier mass (in Mt) for each elevation zone.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ba511cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,5))\n",
    "plt_data.plot.barh(y='Mass', ax=ax)\n",
    "ax.set_xlabel(\"Glacier mass [Mt]\")\n",
    "ax.set_yticks(ax.get_yticks()[::int(100/steps)])\n",
    "ax.set_ylabel(\"Elevation zone [m a.s.l.]\")\n",
    "ax.get_legend().remove()\n",
    "plt.title(\"Initial Ice Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_folder+'NB1_Glacier_Mass_Elevation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b7867",
   "metadata": {},
   "source": [
    "Finally, we need the average glacier elevation Calculate average glacier elevation in meters above sea level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c2bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_glac = round(df_all.altitude.mean(), 2)\n",
    "print(f'Average glacier elevation in the catchment: {ele_glac:.2f} m a.s.l.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972461e",
   "metadata": {},
   "source": [
    "## Store calculated values for other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07b15b",
   "metadata": {},
   "source": [
    "Create a `settings.yml` and store the relevant catchment information. Those information will be used in later notebooks:\n",
    "\n",
    "- **area_cat**: area of catchment in km²\n",
    "- **ele_cat**: average elevation of catchment in m.a.s.l.\n",
    "- **area_glac**: area of glacier in km²\n",
    "- **ele_glac**: average elevation of glaciers in m.a.s.l.\n",
    "- **lat**: latitude of catchment centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f4e9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "settings = {'area_cat': float(area_cat),\n",
    "            'ele_cat': float(ele_cat),\n",
    "            'area_glac': float(area_glac),\n",
    "            'ele_glac': float(ele_glac),\n",
    "            'lat': float(lat)\n",
    "           }\n",
    "with open(output_folder + 'settings.yml', 'w') as f:\n",
    "    yaml.safe_dump(settings, f)\n",
    "\n",
    "print('Settings saved to file.')\n",
    "display(pd.DataFrame(settings.items(),columns=['Parameter','Value']).set_index('Parameter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ab0b1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Download Outputs\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    " The output folder is zipped at the end of each notebook and can be downloaded (file <code>output_download.zip</code>). This is especially useful if you want to use the binder environment again, but don't want to start from notebook #1.</div>\n",
    "\n",
    "<img src=\"images/download_output.png\" width=300>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d5e4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('output_download', 'zip', 'output')\n",
    "print('Output folder can be download now (file output_download.zip)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1faa03e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
